{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#recent-posts","title":"Recent Posts","text":"<p>{{ blog_content }}</p> <p>Have an average day :|</p>"},{"location":"resume/","title":"Hugh Smalley","text":"<p>System Engineer based in South Florida \ud83c\uddfa\ud83c\uddf8 \ud83c\udff4\udb40\udc75\udb40\udc73\udb40\udc66\udb40\udc6c\udb40\udc7f</p> <p>Email / Website / LinkedIn / Github</p>"},{"location":"resume/#engineering-experience","title":"\ud83d\udc68\u200d\ud83d\udcbb Engineering Experience","text":"<p>Sr System Engineer @ LexisNexis Risk Solutions (2018 - Present) HPCC Operations</p> <ul> <li>Operating systems and automation expert</li> <li>Writing code, documentation, and tools to deploy super computing clusters exceeding 10,000 nodes</li> <li>Technologies used: RHEL, CentOS, Rocky, Python, Ansible, AWX, TestInfra, Kubernetes, MySQL, Redis, Stackstorm, Grafana, Prometheus, Azure, Terraform, Vault, AWS, HPCC Systems</li> </ul> <p>Director of Information Technology @ Conrad &amp; Scherer (2016-2018) Legal Firm specialized in complex trials</p> <ul> <li>Automated &amp; Implemented IT processes such as:</li> <li>Inventory &amp; access control</li> <li>Machine learning cluster management</li> <li>Monitoring &amp; data security</li> <li>Ticketing &amp; auditing</li> <li>Network operations &amp; systems automation</li> <li>Technologies used: CentOS, Windows, Python, Ansible, OpenNMS, SnipeIT, Redis, Azure, Dell/EMC SANs, GlusterFS, Samba, WinRM, Powershell, OVirt, Openshift</li> </ul> <p>Consultant &amp; Engineer @ Technical Cabbages INC (2014-2017)  Owner</p> <ul> <li>Specialized in B2B optimizations and process automation</li> <li>Migration of legacy systems to Linux &amp; FOSS based solutions</li> <li>Data backup and ransomeware mitigation</li> <li>Operating system deployments and system automations</li> </ul> <p>System Engineer @ US Epperson Underwriting (2006-2015)  Information Technology Department</p> <ul> <li>Operating system deployment and automation expert</li> <li>Network operation</li> <li>Storage management</li> <li>Tool creation based on powershell and c#</li> </ul>"},{"location":"resume/#languages","title":"\ud83d\udcac Languages","text":"<p>\ud83c\uddfa\ud83c\uddf8 English: Native  \ud83d\udfe9 Esperanto: Conversational-ish \ud83c\udde8\ud83c\uddfa Spanish: Beginner</p> <p>\ud83c\udde8C/C++:Beginner \ud83e\udd80 Rust: Beginner \ud83d\udc0d Python: Intermediate \ud83d\udc1a Shell: Expert \u26a1 PowerShell: Expert \ud83c\uddfe YAML: Expert \ud83c\uddf2 Markdown: Expert</p>"},{"location":"resume/#hobbies","title":"\ud83d\udd79\ufe0f Hobbies","text":"<p>\ud83c\udfb2 Table Top Games\ud83c\udfae Video Games\ud83e\udd16 Microcontrollers\ud83d\udcbe Programing\ud83c\udf99\ufe0f HAM Radio\ud83d\udc68\u200d\ud83c\udf73 Cooking</p>"},{"location":"resume/#volunteering","title":"\ud83d\udccc Volunteering","text":"<p>Lexis Nexis Cares - (2019-Present) Boy Scouts of America - (1999-2003) Special Olympics - (2000-2003)</p>"},{"location":"resume/#accomplishments","title":"\ud83c\udfc6 Accomplishments","text":"<p>Google Knol - (2009) Awarded for best article on Unified Windows Image Deployment Technology Student of the Year - (2003)</p>"},{"location":"resume/#education","title":"\ud83d\udc68\u200d\ud83c\udf93 Education","text":"<p>A+, Network+, Security+, Linux+ CompTIA (exp 2013)</p> <p>MOS, MCSA Microsoft (exp 2010)</p> <p>CCNA Cisco Systems (exp 2006)</p> <p>IPv6 Certified Hurricane Electric (no exp)</p> <p>Miami Dade College - Information Systems Wolfson Campus, Miami Florida, (2003)</p> <p>High School in Information Technology Miami Central Sr High AOIT, Miami Florida, (1999-2003)</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#ad","title":"ad","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#afp","title":"afp","text":"<ul> <li>AFP</li> </ul>"},{"location":"tags/#afpfs","title":"afpfs","text":"<ul> <li>AFP</li> </ul>"},{"location":"tags/#airprint","title":"airprint","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#ansible","title":"ansible","text":"<ul> <li>Ansible</li> </ul>"},{"location":"tags/#apple","title":"apple","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#appletalk","title":"appletalk","text":"<ul> <li>AFP</li> </ul>"},{"location":"tags/#apptainer","title":"apptainer","text":"<ul> <li>Apptainer</li> <li>HPC</li> </ul>"},{"location":"tags/#arch","title":"arch","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#audio","title":"audio","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#backups","title":"backups","text":"<ul> <li>Restic</li> </ul>"},{"location":"tags/#bash","title":"bash","text":"<ul> <li>Bash</li> <li>TMUX</li> </ul>"},{"location":"tags/#centos","title":"centos","text":"<ul> <li>LXC - Image Creation</li> </ul>"},{"location":"tags/#chicken","title":"chicken","text":"<ul> <li>Marry Me Chicken</li> </ul>"},{"location":"tags/#chocolatey","title":"chocolatey","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#cluster","title":"cluster","text":"<ul> <li>Apptainer</li> <li>HPC</li> <li>Warewulf</li> </ul>"},{"location":"tags/#containers","title":"containers","text":"<ul> <li>LXC</li> </ul>"},{"location":"tags/#cups","title":"cups","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#debian","title":"debian","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#dns","title":"dns","text":"<ul> <li>DNS</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>DOCKER</li> <li>Containers</li> </ul>"},{"location":"tags/#domain","title":"domain","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#dotfiles","title":"dotfiles","text":"<ul> <li>dotfiles</li> </ul>"},{"location":"tags/#ffmpeg","title":"ffmpeg","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#freeipa","title":"freeipa","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#fresh","title":"fresh","text":"<ul> <li>dotfiles</li> </ul>"},{"location":"tags/#frontmatter","title":"frontmatter","text":"<ul> <li>Markdown Metadata, its meta-tastic!</li> </ul>"},{"location":"tags/#gaming","title":"gaming","text":"<ul> <li>STEAM</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>GIT</li> <li>SSH</li> </ul>"},{"location":"tags/#hpc","title":"hpc","text":"<ul> <li>Apptainer</li> <li>HPC</li> <li>Warewulf</li> </ul>"},{"location":"tags/#icmp","title":"icmp","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#idm","title":"idm","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#ip6","title":"ip6","text":"<ul> <li>IPv6</li> </ul>"},{"location":"tags/#ipa","title":"ipa","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#ipfs","title":"ipfs","text":"<ul> <li>This is My Second IPFS Post</li> <li>This is My First IPFS Post</li> </ul>"},{"location":"tags/#ipp","title":"ipp","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#ipv6","title":"ipv6","text":"<ul> <li>IPv6</li> </ul>"},{"location":"tags/#json","title":"json","text":"<ul> <li>JSON</li> </ul>"},{"location":"tags/#k8s","title":"k8s","text":"<ul> <li>K8S</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>K8S</li> </ul>"},{"location":"tags/#linux","title":"linux","text":"<ul> <li>LXC - Image Creation</li> <li>MDRAID</li> <li>SELINUX</li> <li>STEAM</li> </ul>"},{"location":"tags/#lxc","title":"lxc","text":"<ul> <li>LXC - Image Creation</li> <li>Containers</li> </ul>"},{"location":"tags/#lxd","title":"lxd","text":"<ul> <li>LXC - Image Creation</li> </ul>"},{"location":"tags/#macos","title":"macos","text":"<ul> <li>Containers</li> <li>macOS</li> </ul>"},{"location":"tags/#makemkv","title":"makemkv","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#mariadb","title":"mariadb","text":"<ul> <li>MYSQL</li> </ul>"},{"location":"tags/#markdown","title":"markdown","text":"<ul> <li>Markdown Metadata, its meta-tastic!</li> </ul>"},{"location":"tags/#mdraid","title":"mdraid","text":"<ul> <li>MDRAID</li> </ul>"},{"location":"tags/#metadata","title":"metadata","text":"<ul> <li>Markdown Metadata, its meta-tastic!</li> </ul>"},{"location":"tags/#mtr","title":"mtr","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#mysql","title":"mysql","text":"<ul> <li>MYSQL</li> </ul>"},{"location":"tags/#network","title":"network","text":"<ul> <li>NETWORKING</li> </ul>"},{"location":"tags/#networking","title":"networking","text":"<ul> <li>IPv6</li> <li>MTR</li> <li>NETWORKING</li> </ul>"},{"location":"tags/#openssl","title":"openssl","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#osx","title":"osx","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#podman","title":"podman","text":"<ul> <li>Containers</li> </ul>"},{"location":"tags/#powershell","title":"powershell","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#printers","title":"printers","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#putty","title":"putty","text":"<ul> <li>SSH</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>Python</li> </ul>"},{"location":"tags/#raid","title":"raid","text":"<ul> <li>MDRAID</li> </ul>"},{"location":"tags/#recipes","title":"recipes","text":"<ul> <li>Marry Me Chicken</li> </ul>"},{"location":"tags/#reddit","title":"reddit","text":"<ul> <li>Marry Me Chicken</li> </ul>"},{"location":"tags/#restic","title":"restic","text":"<ul> <li>Restic</li> </ul>"},{"location":"tags/#rhel","title":"rhel","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#rocky","title":"rocky","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#security","title":"security","text":"<ul> <li>SELINUX</li> </ul>"},{"location":"tags/#sed","title":"sed","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#selinux","title":"selinux","text":"<ul> <li>SELINUX</li> </ul>"},{"location":"tags/#shell","title":"shell","text":"<ul> <li>TMUX</li> </ul>"},{"location":"tags/#ssh","title":"ssh","text":"<ul> <li>Ansible</li> <li>SSH</li> <li>TMUX</li> </ul>"},{"location":"tags/#steam","title":"steam","text":"<ul> <li>STEAM</li> </ul>"},{"location":"tags/#storage","title":"storage","text":"<ul> <li>MDRAID</li> </ul>"},{"location":"tags/#sudo","title":"sudo","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#svn","title":"svn","text":"<ul> <li>SVN</li> </ul>"},{"location":"tags/#tar","title":"tar","text":"<ul> <li>TAR</li> </ul>"},{"location":"tags/#taskfile","title":"taskfile","text":"<ul> <li>Taskfile.yml</li> </ul>"},{"location":"tags/#tcp","title":"tcp","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#terminal","title":"terminal","text":"<ul> <li>TMUX</li> <li>Taskfile.yml</li> </ul>"},{"location":"tags/#tls","title":"tls","text":"<ul> <li>TLS</li> </ul>"},{"location":"tags/#tmux","title":"tmux","text":"<ul> <li>TMUX</li> </ul>"},{"location":"tags/#traceroute","title":"traceroute","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#ubuntu","title":"ubuntu","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#udp","title":"udp","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#vi","title":"vi","text":"<ul> <li>VIM</li> </ul>"},{"location":"tags/#video","title":"video","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#vim","title":"vim","text":"<ul> <li>VIM</li> </ul>"},{"location":"tags/#warewulf","title":"warewulf","text":"<ul> <li>HPC</li> <li>Warewulf</li> </ul>"},{"location":"tags/#windows","title":"windows","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#winget","title":"winget","text":"<ul> <li>SSH</li> <li>Windows</li> </ul>"},{"location":"tags/#wip","title":"wip","text":"<ul> <li>K8S</li> <li>SELINUX</li> </ul>"},{"location":"tags/#wtf","title":"wtf","text":"<ul> <li>K8S</li> </ul>"},{"location":"blog/","title":"\ud83c\udf1f Hello there \ud83d\udd96","text":"<ul> <li>\ud83d\udd2d I\u2019m currently working on HPC &amp; Cloud</li> <li>\ud83c\udf31 I\u2019m currently learning Rust &amp; Warewulf</li> <li>\ud83d\udc6f I\u2019m looking to collaborate on HPC, Ansible, Scripting &amp; Automation, Rocky Linux (Or any EL Linux)</li> <li>\ud83e\udd14 I\u2019m looking for help with Warewulf, Rocky Linux</li> <li>\ud83d\udcac Ask me about HPC, OS Deployments, and Systems Automation! THAT'S MY JAM!</li> <li>\ud83d\udceb How to reach me: Submit an Issue, Email me, or 146.52 MHZ</li> <li>\ud83d\ude04 Pronouns: He/Him</li> <li>\u26a1 Fun fact: I solved a rubik's cube\u2026 by accident\u2026 once</li> </ul> <p>Here's a few popular things I know about: Interested in knowing more?</p> <p></p>"},{"location":"blog/IPFS%20Second%20Post/","title":"This is My Second IPFS Post","text":"<p>Dumped everything and started over with mkdocs + mkdocs-material.</p> <p>You can reach this site a few different ways via ipfs</p> <ul> <li>w4ugh.radio</li> <li>blog.techcabbage.xyz</li> <li>IPNS via dweb.link</li> </ul> <p>All three use ipns and the first two use it via cloudflare. The last one via ipfs.io / dweb.link.</p> <p>No pinning services are paid for so you get what you get. :|</p>","tags":["ipfs"]},{"location":"blog/IPFS%20Second%20Post/#web3","title":"Web3","text":"<p>To make everything faster I'm using github pages and I'm also using cloudflare pages. This means that the content should be accessible via web2 and web3. Which I think is pretty damn neat! So whenever I push a commit to github everything rebuilds and deploys. Even my selfhosted ipfs pinning services!</p>","tags":["ipfs"]},{"location":"blog/IPFS%20Second%20Post/#ipfs-pinning","title":"IPFS Pinning","text":"<p>As mentioned above, have selfhosted pinning. That's just a fancy way of saying I'm running ipfs and have a set of cron jobs to blast my stuff out to the network.</p> <pre><code>@weekly ipfs bitswap reprovide  \n@monthly ipfs repo gc &gt;/dev/null 2&gt;&amp;1  \n@hourly ipfs pin add -r /ipns/k51qzi5uqu5djjziu1k5ty1a4xo13pn4bs4at2hk0u6f9bdllv5k47bqvv0yz9 &gt;/dev/null 2&gt;&amp;1  \n@hourly ipfs refs /ipns/w4ugh.radio | awk {print $1}' | while IFS= read -r line; do ipfs pin add -r ${line}; done  \n@monthly ipfs ls /ipns/w4ugh.radio | awk '{print $1}' | while IFS= read -r line; do ipfs dht provide -r ${line}; done &gt;/dev/null 2&gt;&amp;1\n</code></pre> <p>I have ZERO ideas if this is the right or best way to do this. I could be COMPLETELY wrong so don't rely on this as the right way to do this. I'm sure someone who knows a lot more than I can open an issue or PR on this post with the correct way to do any of that.</p> <p>So YMMV and enjoy!</p>","tags":["ipfs"]},{"location":"blog/Markdown%20Metadata/","title":"Markdown Metadata, It's META-TASTIC","text":"<p>If you ever have needed to include meta with your markdown, take a look at the yaml metadata you can add to markdown. The only one that I've found that isn't supported by everything is tags. Otherwise everything just seems to work great</p> <pre><code>---\ntitle: Markdown Metadata, its meta-tastic!\nauthor: Hugh Smalley\naffiliation: https://hsmalley.github.io\ndate: 2023-03-20\ncopyright: CC-BY-SA\ncomment:\ntags: [tag, tag2]\n---\n</code></pre> <p>You can also do tags like this</p> <pre><code>tags:\n  - tag 1\n  - tag 2\n</code></pre> <p>Using metadata will let you organize and do things that you didn't think were possible with markdown. If you use something like Obsidian then you can pair it with dataview and fly away on some a magic markdown carpet!</p>","tags":["markdown","metadata","frontmatter"]},{"location":"blog/ipfs-first-post/","title":"This is My IPFS First Post","text":"<p>Well first post to this block. I retired my old blog after letting it limp along for years. I'm ready for a new fresh start. So I figured why not use IPFS and Hugo? I'm pretty sure it should be easy to do. Though I'm seeing that Jekyll might be better. I'm not well versed in Ruby so I'm trying to avoid it.</p> <p>So what is this space going being used for? Mostly just geeky after action reports I suppose. My hobbies have expanded and changed a lot over years. So I'm guessing this will have things related to microcontrollers, general sysadmin stuff, python, ham radio, and cooking.</p> <p>Right now I'm curious how much of my old blog I can bring over that is still relevant, how well IPFS handles things, and if I can somehow tie all of this into packet radio!</p>","tags":["ipfs"]},{"location":"blog/ipfs-first-post/#what-is-ipfs","title":"What is IPFS?","text":"<p>So for those of you wondering WHAT IS IPFS? I'll explain it in a way that you might understand using some words you might already know. It's a peer to peer system that instead of addressing content where it's located eg youtube.com it's addressing content by it's ID. Meaning that a video on youtube might be something like https://youtu.be/dQw4w9WgXcQ would be QmdNBsDWQ1JKrt6ynYuQQpL2NzUVJ5eTLW6bz129S1bLsV on the IPFS network. The advantage to this is that even if youtube were to vanish one day. As long as even ONE peer on the IPFS network has that content then you can get to it. Since throwing around hashes aren't really useful for everyday use that's were IPNS and IPFS gateway's really come in handy. We can set it so that it uses DNS. Kind of like how instead of going to some IP Address like 1.1.1.1 you go to a website eg youtube.com. This is the same kinda thing here, it even allows people who do not have an IPFS client built into their browser eg Opera, Brave, etc. to view the content. On a side note if you want to add IPFS to your browser there's extensions and a desktop client.</p>","tags":["ipfs"]},{"location":"how-to/AFP/","title":"Apple Filing Protocol (AFP)","text":"<p>Runs on port 548</p> <p>Use netatalk to get the latest (final) spec of AFP running on your systems. Uses krb5 if you have it setup. Man pages tell you want to do.</p> <p>The short version is this: Install netatalk &amp; avahi, open the ports on your FW, edit your <code>/etc/netatalk/afp.conf</code> file to add your shares. (Re)Start the netatalk service. Enjoy your shares and time machine. I tested this under Rocky 8 and CentOS 7. YMMV</p> <pre><code>;  \n; Netatalk 3.x configuration file  \n;  \n\n[Global]  \n; enable spotlight\nspotlight = yes\n; play nice with samba\nea = samba  \n\n; If you want to share your home(s)\n;[Homes]  \n;basedir regex = /home  \n\n; enable timemachine for backups\n[timemachine]  \npath = /hot-tub/timemachine/\ntime machine = yes  \nvol size limit = 250000\n; this limits it to 250GB\n\n;[AllTheFiles]  \n;path = /I/EAT/YOUR/FREE/SPACE  \n\n[MyVol1]  \npath = /I/Like/Cheese\n\n[MyVolumename2]  \npath = /Data/Path/Here  \n</code></pre>","tags":["afp","afpfs","appletalk"]},{"location":"how-to/Ansible/","title":"Ansible or How to Mess up AT SCALE!","text":"","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#getting-started","title":"Getting Started","text":"<p>To get started you will need two things: Text Editor, Python 3. You can use whatever text editor you want to use, Visual Studio Code is a popular one that runs almost anywhere. As long as you're comfortable with it, use it. If you can run python and ssh you have everything else you need.</p> <p>Note</p> <ul> <li> <p>It is possible to run this on Windows itself, I do not use Windows so I can not help too much. If you have WSL working then you should be able to everything. If you're having problems we do have other systems that can be used</p> </li> </ul> <p> </p> <p>Ansible is in most package repos, for this we're using ansible-core. I recommend you install it via pip to you can ensure you have the same thing as we use in prod</p> <p>Installing ansible is pretty easy, either use your package manager or pip</p> <pre><code># pip\npip install --user ansible-core\n# Redhat Systems\ndnf install ansible\n# Debian Systems\napt install ansible\n</code></pre> <p>Once you have ansible installed let's run our first command to make sure everything is in good working order <code>ansible -i localhost localhost -m ping</code></p> <p>Success</p> <pre><code>\u276f ansible localhost -m ping\nlocalhost | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>See that was pretty easy!</p>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#my-first-inventory-file","title":"My First Inventory File","text":"<p>The next thing we're going to do is create a very simple inventory.ini file.</p> <p>Example</p> <pre><code>[mygroup_name]\nlocalhost\n</code></pre> <p>This file does one thing, tells ansible that <code>localhost</code> is in <code>my_group</code></p> <p>That's all it does! Let's run our command again, this time using the inventory file.</p> <p>Success</p> <pre><code>\u276f ansible -i inventory.ini my_group -m ping\nlocalhost | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n</code></pre> <p>So we told ansible to use our inventory file <code>-i inventory.ini</code> and that we want to connect to <code>my_group</code> and run the ping module.</p> <p> </p> <p>What if we wanted to run against all the hosts in our inventory file? We would use the special group named <code>all</code></p> <p>[!success]</p> <p><pre><code>\u276f ansible -i inventory.ini all -m ping\nlocalhost | SUCCESS =&gt; {\n   \"ansible_facts\": {\n       \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\n</code></pre> </p> <p>We can run commands too!</p> <p>Success</p> <pre><code>\u276f ansible localhost -m ansible.builtin.command -a \"ls\"\nlocalhost | CHANGED | rc=0 &gt;&gt;\nApplications\nDesktop\nDocuments\nDownloads\nLibrary\nOld Laptop\nOneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\nPictures\nPublic\ngit\ninventory.ini\nsvn\n</code></pre> <p>This told ansible that I want to run <code>ls</code> on <code>localhost</code> Let's try something a little more fun!</p> <p>Success</p> <pre><code>\u276f ansible -i inventory.ini all -m ansible.builtin.command -a \"echo (\u256f\u00b0\u25a1\u00b0)\u256f\ufe35 \u253b\u2501\u253b D O N T P A N I C\"\nlocalhost | CHANGED | rc=0 &gt;&gt;\n(\u256f\u00b0\u25a1\u00b0)\u256f\ufe35 \u253b\u2501\u253b D O N T P A N I C\n</code></pre> <p>What changed there? We told ansible that we want to use the <code>ansible.builtin.command</code> module. Ansible assumes that if you do not specify a module name that you just want to run an adhoc command. So what's a module? Simply put module is a connection of stuff that tells ansible how to do do stuff.</p> <p>Let's take this another step farther, how about we use ansible to make sure my mac does not have the dreaded <code>sl</code> command!</p> <p>Example</p> <pre><code>\u276f ansible -i inventory.ini my_group -m community.general.homebrew -a \"name=sl state=absent\" localhost | SUCCESS =&gt; {\n   \"changed\": false,\n   \"changed_pkgs\": [],\n   \"msg\": \"Package already uninstalled: sl\",\n   \"unchanged_pkgs\": [\n       \"sl\"\n   ]\n}\n</code></pre> <p>We can see that it says <code>\"changed:\" false</code> that means that it was not there. So no worries about being punished for messing up up a <code>ls</code></p> <p>And now let's make sure I have the wonderful <code>toilet</code> command!</p> <p>Example</p> <pre><code>\u276f ansible localhost -m community.general.homebrew -a \"name=toilet state=present\"\nlocalhost | CHANGED =&gt; {\n   \"changed\": true,\n   \"changed_pkgs\": [\n       \"toilet\"\n   ],\n   \"msg\": \"Package installed: toilet\",\n   \"unchanged_pkgs\": []\n}\n</code></pre> <p>So now we know how to use ad hoc commands. We can run them on all the hosts in our inventory file or just a group. What if we want to do more?</p> <p> </p> <p>Using the flag <code>--limit</code> we can limit ansible to one host or just a group of hosts e.g. <code>-- limit hostname.here.local</code></p>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#yaml-what-the-hell-is-a-yaml","title":"Yaml? What the Hell is a Yaml?","text":"<p>YAML stands for YAML Ain\u2019t Markup Language. It's what most things use to store key value pairs in a way that humans can manage and understand without too much help. Pretty much everything with Ansible, (and most cloud stuff), uses yaml. When it comes to ansible it only cares about the content of the file and not the name, so you can use <code>.yml</code> or<code>.yaml</code>it only really matters to the humans (and sometimes Windows)</p> <p> </p> <p>Yaml can be a real pain so do yourself a favor, use a linter <code>pip install --user yamllint yamlfix prettier</code></p> <p>What the hell is a linter?  </p> <p>A linter is something that makes sure you code is functional. It will let you know where things are broken and maybe give you a hint on how to fix it. A formatter can help with that, while It doesn't check if it is actually good or functional it tries to make it readable. So you should be using <code>yamllint</code> to check your files for problems and <code>yamlfix</code> to help fix them automatically. Once your good you can use <code>prettier -w</code> to make it pretty. Don't worry, there's ways of automating this</p> <p>Let's just jump right into the deep end and make an inventory file using yaml. Ideally any inventory file you will be using will be some kind of yaml. It doesn't matter if it's for ansible or kubernetes for example. Let's take a look at a simple inventory.yaml file</p> <p>Example</p> <pre><code>---\nall:\n  hosts:\n    alice:\n    bob:\n</code></pre> <p>Here way have two hosts: alice &amp; bob. If this was in ini it would look like</p> <pre><code>[hosts]\nalice\nbob\n</code></pre> <p>So not much of a different from our first inventory.ini We can check this file with the <code>ansible-inventory</code> command</p> <pre><code>\u276f ansible-inventory -i simple.yml --graph\n@all:\n  |--@ungrouped:\n  |  |--alice\n  |  |--bob\n</code></pre> <p>Nothing really complex so far, let's add some more hosts and groups into our inventory.yml</p> <p>Example</p> <pre><code>--all:\n  hosts:\n    alice:\n    bob:\n  children:\n    group_1:\n      hosts:\n        strangechicken:\n        tumescentcloaca:\n    group_2:\n      hosts:\n        wowzer:\n        imaproblem:\n    group_3:\n      hosts:\n        group_1:\n        alice:\n    group_4:\n      hosts:\n        group_2:\n        bob:\n    group_5:\n      hosts:\n        group_3:\n        group_4:\n</code></pre> <p>As you can see we can have many groups even groups that have other groups inside them. We can keep nesting groups as long as we follow the hosts, children, hosts naming. We can make sure this is a valid inventory by looking at with <code>ansible-inventory</code></p> <pre><code>\u276f ansible-inventory -i inventory.yml --graph\n@all:\n  |--@ungrouped:\n  |--@group_1:\n  |  |--strangechicken\n  |  |--tumescentcloaca\n  |--@group_2:\n  |  |--wowzer\n  |  |--imaproblem\n  |--@group_3:\n  |  |--group_1\n  |  |--alice\n  |--@group_4:\n  |  |--group_2\n  |  |--bob\n  |--@group_5:\n  |  |--group_3\n  |  |--group_4\n</code></pre>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#my-first-playbook","title":"My First Playbook","text":"<p>Now running commands one at a time is fine but it's a lot more fun to have something that does a lot of things on a lot of things. This is where Ansible shines!</p> <p>This is also where things start to get more information dense. So buckle up!</p> <p>Ansible will not change anything that doesn't need to be changed, this is important for ensuring something called idempotency. That means that no matter how many times we run the same thing, we're going to get the same results.</p> <p>Let's take a look at a simple but real playbook: <code>change_genesis.yml</code></p> <p>Example</p> <pre><code>---\n- name: Change Genesis Server\n  hosts: all\n  become: true\n  vars:\n    genesis_ip: 10.194.118.129\n    files_to_change:\n      - /etc/profile.d/genesis.sh\n      - /etc/resolv.conf\n      - /etc/profile.d/yum.sh\n  tasks:\n    - name: Replace Bad Genesis\n      ansible.builtin.replace:\n        dest: \"{{ item }}\"\n        regexp: 10\\.173\\.0\\.129\n        replace: \"{{ genesis_ip }}\"\n      loop: \"{{ files_to_change }}\"\n</code></pre> <p>So what does this do? It may look complicated but it's fairly simple. This says look at these files for this string and replace it with that string.</p> <p>Starting from the top we have <code>name:</code> this is the name of the play. The play is what ansible calls it's group of actions or a single action. All plays should be named starting with a capital letter and end with a letter. It will still work if you do not do this, but it will complain the entire time. Kinda like when you don't put on your seat belt\u2026</p> <p>Next <code>hosts:</code> and <code>become:</code> this is the host, hosts, and or group name to run this on. Most of the time this is going to set to <code>all</code>. <code>become:</code> tells ansible that it needs to use sudo aka root powers to do these tasks. The higher up in the play the more things will use sudo powers.</p> <p>Now <code>vars:</code> is a special:</p> <p>Like <code>become:</code> it can be used most anywhere in a play, or it can reference other yaml files. So if we were to put vars in something like our inventory file it would apply per host, per group, or even for everything! In this case we're only saying that the plays before need these variables. Variables can also be anything we want. If I want foo to have the value of bar then I put <code>foo: bar</code>.</p> <p><code>genesis_ip:</code> is the ip to the genesis server in this case we're using<code>10.194.118.129</code> <code>files_to_change:</code> lists the files that we need to check.</p> <p>Further down we get to <code>tasks:</code> tasks are the plays that that playbook is running. In this case we have single play named <code>Replace Bad Genesis</code>. The module name is <code>ansible.builtin.replace</code> that makes it function like <code>sed</code>. The next lines are telling it where and what. <code>dest:</code> is the destination file, <code>regexp:</code> is a regular expression, and <code>replace:</code> is what is going to replace the matched expression.</p> <p>This play has something special attached to it called a <code>loop:</code> this tells the play that there's going to be more things to do so it will function in a loop, kind of like <code>for x in</code> a bash loop.</p> <p>When you call a variable in you have to use another kind of language aside from yaml. It's a templating language/engine called Jinja2. Templating languages (engines) have been around for long time. They allow us to define how things should look while still allowing us to program-maticly change things. DO NOT WORRY, much like regex, you will not need to know it like you will need to understand yaml.</p> <p>To call variables we're using jinja values. So in this case the files are represented by <code>\"{{ item }}\"</code> and the list of files is represented by <code>\" {{ files_to_change }}\"</code> this is more than likely the most complex jinja you will have to remember. The various linter's will handle the rest if you mess it up. So do not worry to much!</p> <p>Let's look at the same thing done in bash</p> <p>Example</p> <pre><code>sed -i 's/10.173.0129/10.194.118.129/g' /etc/profile.d/genesis.sh\nsed -i 's/10.173.0129/10.194.118.129/g' /etc/resolv.conf\nsed -i 's/10.173.0129/10.194.118.129/g' /etc/profile.d/yum.sh\n</code></pre> <p>Now lets look at a script to do the same thing</p> <p>Example</p> <pre><code>#!/bin/bash\ngenesis_ip=10.194.119.129\nbad_genesis=10.173.0.129\nfiles=(/etc/profile.d/yum.sh, /etc/resolv.conf, /etc/profile.d/yum.sh)\nfor file in ${files[@]}; do\n  sed -i \"s/${bad_genesis}/${genesis_ip}/g\" \"${file}\"\ndone\n</code></pre> <p>You could also use the get a bigger hammer approach too <code>grep -rl \"10.173.0.129\" | xargs sed 's/10.173.0.129/10.194.118.129/g'</code> there's a lot of ways to do the same thing. So why do it with Ansible? Well idempotency aside we do not have to worry about what system is on the other end. That same module will work the same on Linux, Mac, Windows, BSD, etc. We can also chain multiple playbooks together in ways that trying to string together multiple shell scripts start to fail at. Radssh is an incredible tool but it will only get you so far. Ansible provides a way to validate your configurations and document them in ways that other methods just simply can not do.</p>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#digressions-aside-how-do-you-actually-run-the-damn-thing","title":"Digressions Aside; how Do You Actually Run the Damn Thing!?","text":"<p>&gt; <code>ansible-playbook -i inventory.yml all -u USERNAME -k change_genesis.yml</code></p> <p>Let's break this down. <code>ansible-playbook</code> is the command that handles playbooks, kind of like we used <code>ansible-inventory</code> to look at out inventory file. <code>-i inventory.yml all</code> says use all the hosts in the inventory file. <code>-u USERNAME -k</code> says use this username and prompt for a sudo password so it can be used when needed. Some depending on how sudoers is configured you can leave this out. For most admin/op account <code>-k</code> isn't needed. Lastly the name of the playbook <code>change_genesis.yml</code></p>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#how-to-write-an-ansible-playbook","title":"How-to: Write an Ansible Playbook","text":"","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#the-general-layout-of-the-a-playbook","title":"The General Layout of the a Playbook","text":"<pre><code>---\n# How to use:\n# ansible-playbook -i inventory/inventory.file.here playbooks/template.yml\n- name: Playbook name\n  hosts: all\n  become: false # Change to true for sudo powers\n  vars: # Any vars needed for this playbook\n  handlers: # Any handlers needed for this playbook\n  pre_tasks: # Any pre-tasks needed for this playbook\n  post_tasks: # Any post-tasks needed for this playbook\n  tasks: # The main body of the playbook\n</code></pre> <p>Playbooks should follow this layout where possible. You may have some things that require more or less.</p>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#formatting-and-general-rules","title":"Formatting and General Rules","text":"","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#general-rules","title":"General Rules","text":"<ul> <li>Every play should be named started with a capital letter</li> <li>Try to to keep lines under 80 col in length</li> <li>Use true or false to maintain truthy (yaml spec)</li> <li>Place tags where it makes sense</li> <li>Place options at the top of a block when possible; this helps with readability</li> <li>Place optionals above the module where possible; this helps with readability</li> <li>Comments should have two spaces # a single space, then the text (yaml &amp; markdown spec)</li> <li>Names should end with a letter where possible</li> <li>Registers &amp; variable names should not contain non alpha characters when possible.</li> <li>Anything that makes changes to the system when run should be handler when at all possible</li> <li>If you have to do ignore errors on something you either have a very niche edge case or it's lazy code. Anything that ignores errors should be a handler too.</li> </ul> <p>Bad Formatting</p> <pre><code>ansible.posix.authorized_key: #Adding OPS Keys to the root for some silly reasons\nuser: root\nstate: present\nkey: \"{{ item }}\"\nloop: \"{{ keys }}\"\nnotify: YouHaveBeenWarned\nignore_errors: yes\n</code></pre> <p>What's wrong with this?</p> <ul> <li>There is no name that starts with a capital letter</li> <li>There is a comment that isn't spaced out correctly and goes way past 80 col.</li> <li>Options are at the bottom rather than the top.</li> <li>Contains ignore_errors and yes</li> <li>No tags</li> </ul> <p>Good Formatting</p> <pre><code>- name: Add OPS Authorized Keys\n  notify: YouHaveBeenWarned\n  tags: [keys, ssh]\n  ansible.posix.authorized_key:\n    user: root\n    state: present\n    key: \"{{ item }}\"\n  loop: \"{{ keys }}\"\n</code></pre> <p>Why is this better?</p> <ul> <li>The task is named clearly and correctly</li> <li>You know right away that it is tagged and will trigger a handler</li> <li>Does not break truthy or ignore errors</li> <li>Does not have an unnecessary or overly long comment</li> </ul>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#formatting-linting","title":"Formatting &amp; Linting","text":"<p>If you follow the general rules then you should be able to format your playbooks with something like <code>prettier</code> and run <code>ansible-lint</code> on them. This will let you know if you have any errors that require fixing. You goal is to need none or as few ignores in your <code>.ansible-lint-ignore</code> file and be as close to the production ready profile as possible.</p> <p> </p> <p>If you encounter problems with your playbook that are formatting related running <code>yamlfix</code> then <code>prettier</code> on them will normally fix them for you.</p> <p>Once your playbook has been formatted and linted you can try it out.</p>","tags":["ansible","ssh"]},{"location":"how-to/Ansible/#the-end-ish","title":"The End-ish","text":"<p>This is has been an overview on Ansible. It is by no means a complete or even good overview. It's purely from my memory and there are bounds to be gaps or things I'm making assumptions about. I will be adding this document to github in the future once I am mostly satisfied with it's content.</p> <p>As with everything in life; YMMV, caveat emptor, pass performance doesn't equal future success, and\u2026</p> <p>Have an Average Day :|</p>","tags":["ansible","ssh"]},{"location":"how-to/Apptainer/","title":"Apptainer","text":"<p>WIP</p>","tags":["hpc","cluster","apptainer"]},{"location":"how-to/Bash/","title":"Bash","text":"<p>.-</p>","tags":["bash"]},{"location":"how-to/Bash/#bash","title":"Bash","text":"","tags":["bash"]},{"location":"how-to/Bash/#bash-file-testing","title":"Bash File Testing","text":"<pre><code>-b filename - Block special file\n-c filename - Special character file\n-d directoryname - Check for directory Existence\n-e filename - Check for file existence, regardless of type (node, directory, socket, etc.)\n-f filename - Check for regular file existence not a directory\n-G filename - Check if file exists and is owned by effective group ID\n-G filename set-group-id - True if file exists and is set-group-id\n-k filename - Sticky bit\n-L filename - Symbolic link\n-O filename - True if file exists and is owned by the effective user id\n-r filename - Check if file is a readable\n-S filename - Check if file is socket\n-s filename - Check if file is nonzero size\n-u filename - Check if file set-user-id bit is set\n-w filename - Check if file is writable\n-x filename - Check if file is executable\n</code></pre> <p>Bash Shell CGI (yolinux.com)</p>","tags":["bash"]},{"location":"how-to/Bash/#quick-tests-in-scripts","title":"Quick Tests in Scripts","text":"<p>If you need to test something and have a single action taken then you can avoid doing a bunch of if statements by doing a single line test. You can use the above tests to change what you're looking for.</p> <pre><code>[[ -f /file/path/here ]] || touch /file/path/here\n[[ -d /dir/path/here ]] || mkdir -p /dir/path/here\n[[ -L /link/path/here ]] || ln -s /file/path/here /link/path/here\n</code></pre> <p>These examples will take no action if true, so if they exist nothing happens. When they're false it will take the action on the other side of ||. You can do the inverse of it too by added a not (!) into in front of the test</p> <pre><code>[[ ! -f /file/path/here ]] || rm -f /file/path/here\n</code></pre> <p>So in this example if the file doesn't exist then it is true and nothing happens, otherwise the file is deleted if it does exist.</p>","tags":["bash"]},{"location":"how-to/Bash/#functions","title":"Functions","text":"<p>Bash (shell) functions are denoted by a keyword, () and {}. In the example below we have 3 functions, with the 4th function calling the other 3\u2026 and yes read into that what you will\u2026</p> <pre><code>there() {\n    echo \"THERE -\"\n}\nare() {\n    echo \"ARE -\"\n}\nfour() {\n    echo \"FOUR LIGHTS\"\n}\npicard-says() {\n    there\n    are\n    four\n}\n\necho \"Picard says:\"\npicard-says # This runs the function picard-says, which runs the others\n</code></pre>","tags":["bash"]},{"location":"how-to/CUPS/","title":"Printers! Oh what Hellish Joy","text":"","tags":["cups","printers","airprint","ipp"]},{"location":"how-to/CUPS/#apple-airprint-w-cups","title":"Apple AirPrint w/ CUPS","text":"<p>Setup your printer(s) in cups and make sure it's working and shared (and sure you have avahi installed too).</p> <p>Now create a service file in <code>/etc/avahi/services</code> that has your printers name. Make sure you're matching this exactly to what you have in cups eg. HL2230 in cups becomes: <code>/etc/avahi/services/AirPrint-HL2230.service</code></p> <p>Here's my jinja template:</p> <pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;!DOCTYPE service-group SYSTEM \"avahi-service.dtd\"&gt;\n&lt;service-group&gt;\n&lt;name replace-wildcards=\"yes\"&gt;AirPrint {{ printer_name }} @ %h&lt;/name&gt;\n&lt;service&gt;\n&lt;type&gt;_ipp._tcp&lt;/type&gt;\n&lt;subtype&gt;_universal._sub._ipp._tcp&lt;/subtype&gt;\n&lt;port&gt;631&lt;/port&gt;\n&lt;txt-record&gt;txtvers=1&lt;/txt-record&gt;\n&lt;txt-record&gt;qtotal=1&lt;/txt-record&gt;\n&lt;txt-record&gt;Transparent=T&lt;/txt-record&gt;\n&lt;txt-record&gt;URF=none&lt;/txt-record&gt;\n&lt;txt-record&gt;rp=printers/{{ printer_name }}&lt;/txt-record&gt;\n&lt;txt-record&gt;note={{ printer_name }}&lt;/txt-record&gt;\n&lt;txt-record&gt;product=(GPL Ghostscript)&lt;/txt-record&gt;\n&lt;txt-record&gt;printer-state=3&lt;/txt-record&gt;\n&lt;txt-record&gt;printer-type=0x2900c&lt;/txt-record&gt;\n&lt;txt-record&gt;pdl=application/octet-stream,application/pdf,application/postscript,application/vnd.cups-raster,image/gif,image/jpeg,image/png,image/tiff,image/urf,text/html,text/plain,application/vnd.adobe-reader-postscript,application/vnd.cups-pdf&lt;/txt-record&gt;\n&lt;/service&gt;\n&lt;/service-group&gt;\n</code></pre> <p>Now restart Avahi (and enable ipp on firewalld) and enjoy having CUPS enabled AirPrint printer even if it's a 25 year old laser printer like mine.</p>","tags":["cups","printers","airprint","ipp"]},{"location":"how-to/DNS/","title":"DNS","text":"<p>[!seealso] Basic master and slave DNS setup with Bind | Jensd's I/O buffer</p>","tags":["dns"]},{"location":"how-to/Docker/","title":"DOCKER","text":"<p>https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-rocky-linux-8 https://ranchermanager.docs.rancher.com/pages-for-subheaders/rancher-on-a-single-node-with-docker</p>","tags":["docker"]},{"location":"how-to/Dotfiles/","title":"Welcome $HOME","text":"","tags":["dotfiles","fresh"]},{"location":"how-to/Dotfiles/#what-are-dotfiles","title":"What Are Dotfiles?","text":"<p>Dot files are the files that start with a period in your home dir. They're pretty much the config files that make your system yours. While they historically they've just been files in the root of your home they're slowly moving to your things like .config</p>","tags":["dotfiles","fresh"]},{"location":"how-to/Dotfiles/#the-basic-dotfiles","title":"The Basic Dotfiles","text":"<p>Almost every <code>*</code>nix system uses <code>/etc/skel</code> to store it's default bland dot files for a system. This is also the directory you put anything you need a new account to have in. So in that dir you'll find things like <code>.bashrc</code>, <code>.zshrc</code>, etc. These are you basic dot files.</p> <p>Most people should be aware of <code>~/.bashrc</code> it's the file that gets parsed whenever you open up a new bash prompt. Other files that you should be aware of are:</p> <ul> <li>.bashrc</li> <li>.bash_history</li> <li>.vimrc</li> <li>.viminfo</li> <li>.screenrc</li> <li>.tmuxrc</li> <li>.netrc</li> </ul> <p>\u2026 and the list goes on. These are the basics though so let's break them down into what they do and why (or why not) to keep them around</p> <p> </p> <p>Pretty much anything with the suffix rc is going to be a config that gets run when something is opened.</p> <p>Starting with the <code>*</code>shrc files. These belong to whatever shell you're using. So bashrc for bash, zshrc for zsh, tcshrc for tcsh, etc. These files are important because they store things that you use on a regular basis. So if you like to alias <code>ll</code> to <code>ls -l</code> you wold add that into your shrc file. If you're going to backup and store these on somewhere like github make sure you do not put anything sensitive in there eg setting an environmental to a password or leaving an API key in there. You shouldn't be doing that anyway, but if you're gonna do it might as well practice harm reduction and store in another file that you source from your shrc file.</p> <p>Next up is your .tmux &amp; .screenrc files. These files contain your settings for screen &amp; tmux. If you seldom use these apps you'll want to make sure that anything you do for their configs are backed up otherwise the next time you need them they might do what you expect them to do.</p> <p>Your .vimrc, .nanorc, etc. are you text editor rc files. If you have any custom settings configure these are going to super important to keep around. Nothing is worse than trying to edit a file and not having your keybinds, plugins, etc not there. It makes things very annoying.</p> <p>Lastly there's some rc files that you should not be backing up. These are things that you never use or customize. You can port them around if you really want to. The only exception to that is the .netrc file.</p> <p> </p> <p>DO NOT BACKUP YOUR <code>.netrc</code> FILES ANYWHERE PUBLIC.</p> <p>Your .netrc file contains user/pass data to login to stuff. You really SHOULD NOT be using it, but there are some reasons to use it for instance curl, wget, youtube-dl, etc. look for this file to login to things. So if you have some edge case for it then that's a good enough reason to keep it around. If you really need it for some one off things maybe symlink it into tmpfs that way it's lost upon reboot?</p> <p>Next are you info and history files.</p> <p> </p> <p>Pretty much anything with the suffix info or history is going to be a history file  </p> <p>These files are rarely ever a good idea to port around. At best it's just junk info that doesn't pertain to whatever system you're on. At worse they could contain sensitive information. Generally avoid moving these around.</p>","tags":["dotfiles","fresh"]},{"location":"how-to/Dotfiles/#keeping-your-dotfiles-fresh","title":"Keeping Your Dotfiles fresh","text":"<p>One of the more fun/painful things of setting up a new system is getting your dotfiles moved over. There's A LOT of ways to organize them and keep them. I've tried almost everything over the years. The last system I used was yadm and while it worked good enough it just never really did it for me. So I would almost always just move things around and redo configs.</p> <p>Recently I've come upon fresh and the workflow for that just makes sense to me. So I'm slowly migrating into this.</p>","tags":["dotfiles","fresh"]},{"location":"how-to/FFMPEG/","title":"Using <code>ffmpeg</code>","text":"","tags":["ffmpeg","makemkv","video","audio"]},{"location":"how-to/FFMPEG/#combine-video-files","title":"Combine Video Files","text":"<p>First make a video stream file. All this is a text file with the file names in listed by <code>file 'filename.mkv'</code> for each file. Here is a quick and easy way to generate that file:</p> <p>Example</p> <pre><code>printf \"file '%s'\\n\" *.mkv &gt; mylist.txt\n</code></pre> <p>Next you need to tell <code>ffmpeg</code> that you're going to combine the file with the <code>concat</code> format. In this example I was converting a DVD I dumped with <code>makemkv</code>. Since they're all using the same formats and resolutions I didn't have to worry about adding a bunch of stuff. I also had it write it to <code>mp4</code> and made me easier for Plex to stream it with the <code>movflags</code>.</p> <p>Example</p> <pre><code>ffmpeg -fflags +genpts -f concat -safe 0 -i mylist.txt -c copy -movflags +faststart -movflags use_metadata_tags Your.File.Name.mp4\n</code></pre>","tags":["ffmpeg","makemkv","video","audio"]},{"location":"how-to/FFMPEG/#removing-tracks-from-a-file","title":"Removing Tracks from a File","text":"<p>First find all the mappings in the file by doing <code>ffmpeg -i video.file.here.mp4</code></p> <p>You should see a few things that start with <code>Stream #0:0</code> or <code>Stream #0:1</code> and so on. Those are the various audio and video streams, yes things like mkv can have many different streams, codecs, etc. mp4, m4v, webm, etc are limited though. So lets' say I have a file that has both English and Spanish audio and I only want to keep the 2nd audio stream and drop the first. Looking at the mappings I can see that <code>0:0</code> is the video stream, so I want to keep that. I also see that I want to keep <code>0:2</code> and drop <code>0:1</code> so I will tell <code>ffmpeg</code> that I want to copy everything in the file, but I want to drop the <code>0:1</code> stream. In this example you can see that I also included flag to make it load faster and use metadata too.</p> <p>Example</p> <pre><code>ffmpeg -i video.file.here.m4v -map 0:0 -map 0:2 -c copy -movflags faststart -movflags use_metadata_tags video.output.here.mp4\n</code></pre>","tags":["ffmpeg","makemkv","video","audio"]},{"location":"how-to/FFMPEG/#setting-new-default-audio-track","title":"Setting New Default Audio Track","text":"<p>Let's remap some audio streams and set the 0:2 to the default</p> <p>Example</p> <pre><code>ffmpeg -c copy -map 0:0 -map 0:2 -map 0:1 -map 0:7 -disposition:a:0 default -disposition:a:1 none\n</code></pre> <p>Looking at the command above we're saying that in the output file audio a:0 will be the default and a:1 will not be anything. This sets whatever was the default to none and promotes the stream we want as the default. You'll see that we're also moving stream 0:2 head of stream 0:1. So in the output file stream 0:2 will in the a:0 place.</p>","tags":["ffmpeg","makemkv","video","audio"]},{"location":"how-to/FFMPEG/#change-the-language-tag-of-a-stream","title":"Change the Language Tag of a Stream","text":"<p>Lets say we have some tracks that are not tagged as the language we want. We'll add the metadata tags BEFORE the mapping. The metadata tags are going to corrospond to the output streams in their order, so taking the command above we would do</p> <p>Example</p> <pre><code>ffmpeg -i video.mp4 -c copy -metadata:s:0 language=eng -metadata:s:1 language=eng -metadata:s:2 language=eng -metadata:s:3 language=eng -map 0:0 -map 0:2 -map 0:1 -map 0:7 -disposition:a:0 default -disposition:a:1 none remapped_video.mp4\n</code></pre> <p>This command says that stream 0,1,2,3 are going to have the tag eng and we're mapping streams 0,2,1,7 from the input to and they will become streams 0,1,2,3 on the output with the default audio stream being the 0:2 on the input or 0:1 on the output.</p> <p>Let's say we want to add other information such as title information, we would add another <code>-metadata:s:0 title=\"TITLE HERE\"</code> for each stream. here's an example of remapping, dropping, and adding metadata</p> <p>Example</p> <pre><code>ffmpeg -i Highlander\\ \\(1986\\)\\ Remux-1080p\\ AV1\\ DTS-HD\\ MA\\ \\[EN+DE\\]\\ 4K4U\\ tt0091203.m4v -y -c copy -metadata:s:0 title=\"Highlander (1986) Remux-1080p x265 DTS-HD MA [E tt0091203\" -metadata:s:1 title=\"DTS-HD MA 5.1\" -metadata:s:1 language=eng -metadata:s:2 title=\"DTS-MA 2.0\" -metadata:s:2 language=eng -metadata:s:3 title=\"DTS 2.0\" -metadata:s:3 language=eng -map 0:0 -map 0:2 -map 0:1 -map 0:7 -disposition:a:0 default -disposition:a:1 none Highlander\\ \\(1986\\)\\ Remux-1080p\\ AV1\\ DTS-HD\\ MA\\ \\[EN\\]\\ tt0091203.mp4\n</code></pre> <p>Success</p> <pre><code>Output #0, mp4, to 'Highlander (1986) Remux-1080p AV1 DTS-HD MA [E tt0091203.mp4':\n\u00a0Metadata:\n\u00a0\u00a0\u00a0major_brand \u00a0\u00a0\u00a0\u00a0: mp42\n\u00a0\u00a0\u00a0minor_version \u00a0\u00a0: 512\n\u00a0\u00a0\u00a0compatible_brands: mp42av01iso2mp41\n\u00a0\u00a0\u00a0title \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0: Highlander (1986) Remux-1080p x265 DTS-HD MA [EN+D 4K4U tt0091203\n\u00a0\u00a0\u00a0encoder \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0: Lavf58.29.100\n\u00a0\u00a0\u00a0Stream #0:0(und): Video: av1 (Main) (av01 / 0x31307661), yuv420p10le(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], q=2-31, 7534 kb/s, 24 fps, 24 tbr, 90k tbn, 90k tbc (default)\n\u00a0\u00a0\u00a0Metadata:\n\u00a0\u00a0\u00a0\u00a0\u00a0creation_time \u00a0\u00a0: 2023-02-28T21:31:32.000000Z\n\u00a0\u00a0\u00a0\u00a0\u00a0handler_name \u00a0\u00a0\u00a0: VideoHandler\n\u00a0\u00a0\u00a0\u00a0\u00a0title \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0: Highlander (1986) Remux-1080p x265 DTS-HD MA [E tt0091203\n\u00a0\u00a0\u00a0Stream #0:1(eng): Audio: dts (DTS-HD MA) (mp4a / 0x6134706D), 48000 Hz, 5.1(side), s32p (24 bit) (default)\n\u00a0\u00a0\u00a0Metadata:\n\u00a0\u00a0\u00a0\u00a0\u00a0creation_time \u00a0\u00a0: 2023-02-28T21:31:32.000000Z\n\u00a0\u00a0\u00a0\u00a0\u00a0title \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0: DTS-HD MA 5.1\n\u00a0\u00a0\u00a0\u00a0\u00a0handler_name \u00a0\u00a0\u00a0: Surround\n\u00a0\u00a0\u00a0Stream #0:2(eng): Audio: dts (DTS-HD MA) (mp4a / 0x6134706D), 48000 Hz, stereo, s16p\n\u00a0\u00a0\u00a0Metadata:\n\u00a0\u00a0\u00a0\u00a0\u00a0creation_time \u00a0\u00a0: 2023-02-28T21:31:32.000000Z\n\u00a0\u00a0\u00a0\u00a0\u00a0title \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0: DTS-MA 2.0\n\u00a0\u00a0\u00a0\u00a0\u00a0handler_name \u00a0\u00a0\u00a0: Stereo\n\u00a0\u00a0\u00a0Stream #0:3(eng): Audio: dts (DTS) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 768 kb/s\n\u00a0\u00a0\u00a0Metadata:\n\u00a0\u00a0\u00a0\u00a0\u00a0creation_time \u00a0\u00a0: 2023-02-28T21:31:32.000000Z\n\u00a0\u00a0\u00a0\u00a0\u00a0title \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0: DTS 2.0\n\u00a0\u00a0\u00a0\u00a0\u00a0handler_name \u00a0\u00a0\u00a0: Stereo\nStream mapping:\n\u00a0Stream #0:0 -&gt; #0:0 (copy)\n\u00a0Stream #0:2 -&gt; #0:1 (copy)\n\u00a0Stream #0:1 -&gt; #0:2 (copy)\n\u00a0Stream #0:7 -&gt; #0:3 (copy)\nPress [q] to stop, [?] for help\n</code></pre>","tags":["ffmpeg","makemkv","video","audio"]},{"location":"how-to/FFMPEG/#m1-hardware-acceleration","title":"M1 Hardware Acceleration","text":"<p>Apple silicon uses <code>videotoolbox</code> for it's HW Acceleration. If you need to control the quality add <code>-q:v 50</code> to after <code>_videotoolbox</code> with the number being 0-100 where 100 is lossless.</p> <p>The M1 supports the following encode / decode codes:</p> <ul> <li><code>mpeg1_videotoolbox</code></li> <li><code>mpeg2_videotoolbox</code></li> <li><code>mpeg4_videotoolbox</code></li> <li><code>h263_videotoolbox</code></li> <li><code>h264_videotoolbox</code></li> <li><code>prores_videotoolbox</code></li> <li><code>vp9_videotoolbox</code></li> <li><code>hevc_videotoolbox</code></li> </ul> <p>Example</p> <pre><code>ffmpeg -i video_file.mp4 -c:v h264_videotoolbox -c:a copy -vf subtitles=subititles.vtt -movflags faststart -movflags use_metadata_tags video_burnin_subtitles.mp4\n</code></pre> <p>This example shows how to burn in subtitles to a video using hardware acceleration on Apple silicon, e.g. M1, while placing the ATOM upfront to easy decode / streaming.</p> <p>See Also: This Stack Exchange post Documentation on the <code>-map</code> flag</p>","tags":["ffmpeg","makemkv","video","audio"]},{"location":"how-to/FreeIPA/","title":"FreeIPA","text":"","tags":["ipa","freeipa","idm","domain","ad"]},{"location":"how-to/FreeIPA/#joining-a-domain-ipaad","title":"Joining a Domain (IPA/AD)","text":"<p>Joining a domain is pretty simple just run:</p> <pre><code>realm join --verbose ipa.example.com\n</code></pre> <p>Then to check to make sure everything is working:</p> <pre><code>getent passwd admin@ipa.example.com\n</code></pre> <p> </p> <p>The <code>realm</code> tool should install the needed packages to join your domain. However the common packages are - oddjob, oddjob-mkhomedir, sssd-common, and freeipa-client. To speed up the process you can pre-install those packages.</p> <pre><code>dnf install -y oddjob oddjob-mkhomedir sssd-common freeipa-client\n</code></pre>","tags":["ipa","freeipa","idm","domain","ad"]},{"location":"how-to/FreeIPA/#the-easy-way-with-ansible","title":"The Easy Way; with Ansible","text":"<p>You can join a bunch of systems all at once using ansible.</p> <pre><code>---\n- name: Join Hosts to IPA Domain\n  hosts: all\n  tags: [freeipa, domain]\n  vars_files:\n  vars:\n    realm_packages:\n      - oddjob\n      - sssd-common\n      - freeipa-client\n      - oddjob-mkhomedir\n  vars_prompt:\n    - name: leave_domain\n      prompt: Do you want to leave the domain? (true/false)\n      default: false\n      private: false\n    - name: join_domain\n      prompt: Do you want to join the domain? (true/false)\n      default: true\n      private: false\n    - name: ipa_domain\n      prompt: \"Enter IPA Domain\"\n      private: false\n    - name: ipa_username\n      prompt: \"Enter IPA Username\"\n      private: false\n    - name: ipa_password\n      prompt: \"Enter Password\"\n      private: true\n  handlers:\n    - name: Reboot\n      ansible.builtin.reboot:\n  pre_tasks:\n    - name: Install Required Packages\n      ansible.builtin.dnf:\n        name: \"{{ realm_packages }}\"\n        state: present\n  post_tasks:\n  tasks:\n    - name: Joining System(s)\n      when: join_domain | bool\n      notify: Reboot\n      block:\n        - name: Join system to domain \"{{ ipa_domain }}\"\n          ansible.builtin.expect:\n            command: /bin/bash -c \"/usr/sbin/realm join -v --user={{ ipa_username }} {{ ipa_domain }}\"\n            responses:\n              Password for *: \"{{ ipa_password }}\"\n      rescue:\n        ansible.builtin.debug:\n          msg: \"This will fail when run a second time, ensure the systems are not joined already\"\n    - name: Leaving System(s)\n      when: leave_domain | bool\n      notify: Reboot\n      block:\n        - name: Join system to domain \"{{ ipa_domain }}\"\n          ansible.builtin.expect:\n            command: /bin/bash -c \"/usr/sbin/realm leave -v --user={{ ipa_username }} {{ ipa_domain }}\"\n            responses:\n              Password for *: \"{{ ipa_password }}\"\n      rescue:\n        ansible.builtin.debug:\n          msg: \"This will fail when run a second time, ensure the systems are have not left already\"\n</code></pre>","tags":["ipa","freeipa","idm","domain","ad"]},{"location":"how-to/GIT/","title":"GIT","text":"","tags":["git"]},{"location":"how-to/GIT/#pull-all-the-things","title":"Pull ALL THE THINGS","text":"<p>Example</p> <pre><code>git branch -r | grep -v '\\-&gt;' | sed \"s,\\x1B\\[[0-9;]*[a-zA-Z],,g\" | while read remote; do git branch --track \"${remote#origin/}\" \"$remote\"; done\ngit fetch --all\ngit pull --all\n</code></pre>","tags":["git"]},{"location":"how-to/GIT/#clone-a-bare-repo-useful-for-hosting","title":"Clone a Bare Repo, Useful for Hosting","text":"<p><code>git clone --bare --mirror --shared URI-TO-REPO</code></p> <p>If you're hosting mirrors that are done as bare repos, then you should update. This makes sure you're getting all the branches and updates. Drop this into a cron and let it run.</p> <pre><code>#!/usr/bin/env bash\ncd /srv/git || exit 1\nfor x in $(echo *.git); do\n        cd \"${x}\"\n        echo \"Updating ${x}\"\n        git remote update\n        git --bare fetch --all\n        git --bare fetch origin *:*\n        cd ..\ndone\n</code></pre>","tags":["git"]},{"location":"how-to/GIT/#convert-a-repo-to-bare-useful-when-a-big-oopsie-happens","title":"Convert a Repo to Bare - Useful when a Big Oopsie Happens","text":"<p>Example</p> <pre><code>cd repo\nmv .git ../repo.git # renaming just for clarity\ncd ..\nrm -fr repo\ncd repo.git\ngit config --bool core.bare true\n</code></pre>","tags":["git"]},{"location":"how-to/GIT/#git-commits-failing-to-sign","title":"Git Commits failing to Sign","text":"<p>This most likely happens when pin entry is broken</p> <p>When pinentry-mac is most likely broken</p> <pre><code>brew upgrade gnupg\nbrew link --overwrite gnupg\nbrew install pinentry-mac\necho \"pinentry-program /opt/homebrew/bin/pinentry-mac\" &gt;&gt; ~/.gnupg/gpg-agent.conf\nkillall gpg-agent\n</code></pre> <p> </p> <p>Github's getting quick guide to git</p>","tags":["git"]},{"location":"how-to/HPC/","title":"HPC","text":"<p>This is my guide and notes on how to get started with the world of HPC. This is going to focus on using open source industry standard tooling for creating HPC clusters. Since the internal tools I use in my day job aren't open source I'll be using Warewulf, it's also my preferred way of getting clusters going.</p> <p>So sit back and bookmark. This is going to be a LONG one\u2026</p>","tags":["hpc","cluster","warewulf","apptainer"]},{"location":"how-to/HPC/#warewulf","title":"[[Warewulf]]","text":"<p>WIP</p>","tags":["hpc","cluster","warewulf","apptainer"]},{"location":"how-to/HPC/#apptainer","title":"[[Apptainer]]","text":"<p>WIP</p>","tags":["hpc","cluster","warewulf","apptainer"]},{"location":"how-to/IPV6/","title":"IPv6","text":"","tags":["ipv6","ip6","networking"]},{"location":"how-to/IPV6/#ipv6-tokens","title":"IPv6 Tokens","text":"<p>IPv6 tokens are used when you want use your dynamic prefix but have a static suffix. So for example I want to a system to always end with <code>::dead:beef</code> this will give me a static address to use even if/when the prefix changes! I use this on servers and other devices that I want/need a static IP but still want to reach them via IPv6.</p> <pre><code>nmcli connection modify eno1 ipv6.addr-gen-mode eui64\nnmcli connection modify eno1 ipv6.token ::dead:beaf\nsystemctl restart NetworkManager.service\n</code></pre> <p>Keep in mind that <code>eno1</code> is the name of the connection profile, which is typically named after the interface. So if you have a different interface/profile name you'll need to change it match that.</p> <p>Another thing to take notice of is that eui64 is not compatible with global privacy mode. So this isn't something that you would want to enabled on something like your laptop/desktop that you're browsing the internet/gaming/etc. Unless you know what you're doing and/or want that. YMMV / Caveat Emptor / Yadda yadda\u2026</p>","tags":["ipv6","ip6","networking"]},{"location":"how-to/JSON/","title":"Using JSON","text":"","tags":["json"]},{"location":"how-to/JSON/#ahoy-jq","title":"Ahoy <code>jq</code>","text":"<p>Let's parse some json with <code>jq</code>. First we're going to need some json to work with. Let's use <code>ffprobe</code> to dump some from a video file</p> <p>Example</p> <pre><code>ffprobe -v quiet -print_format json -show_streams \"${video}\" &gt;\"${video}.json\"\n</code></pre> <p>Now let's take a look at the json, we can either use the file we created or pipe it directly into <code>jq</code></p> <p>Example</p> <pre><code>ffprobe -v quiet -print_format json -show_streams \"${video}\" | jq\n# OR\njq &lt; \"${video}\"\n</code></pre> <p>Let's parse the data in a few ways, ending with only selecting English subtitles. These methods can be mixed and matched as needed to get the information you require from json</p> <p>Example</p> <pre><code># Get information on the first stream in the video file\njq -r '.streams[0]' &lt;${video}\n# Get all streams\njq -r '.streams[]' &lt;${video}\n# Return only the index number of the streams\njq -r '.streams[] | .index' &lt;${video}\n# Return the index, codec name, and codec type of all video streams.\njq -r '.streams[] | {index, codec_name, codec_type' &lt;${video}\n# Return the index and codec name of only the subtitle streams\njq -r '.streams[] | select(.codec_type==\"subtitles\") | {index, codec_name}' &lt;${video}\n# Return only the subtitle streams that have a language tag\njq -r '.streams[] | select(.codec_type==\"subtitles\") | select(.tags.language)' &lt;${video}\n# Return only the subtitle streams that contain \"en\" in their language tag\njq -r '.streams[] | select(.codec_type==\"subtitles\") | select(.tags.language | contains(\"en\"))' &lt;${video}\n</code></pre>","tags":["json"]},{"location":"how-to/Kubernetes/","title":"Kubernetes","text":"","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#bare-metal-the-hard-way","title":"Bare Metal; the Hard-way","text":"<p>I've often said the best way to learn something is to teach it. The best way to teach it is to understand it. To understand it, you gotta build it. At least that's how I do most things in IT. I can't say I fully understand everything about Kubernetes but I'm damn close to understanding the basics. I understand that this isn't going to be for most people and that most people are completely happy with never building this on bare metal. After all you have to be a little crazy to this. HOWEVER if you actually do it, you'll learn a lot, or at least I did.</p>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#using-k0s-to-get-closer-to-raw-k8s","title":"Using K0S to Get Closer to Raw K8S","text":"<p>So I'm going to take a shortcut and cheat a little. While yes, I can totally build out k8s using upstream packages. I've found for my purposes using k0s is damn near identical and a hell of a lot faster to get started. While some people may jump into something like k3s, microk8s, etc. they abstract a lot of stuff away from you. For example, do you know how to flannel works? Have you ever deployed it? When it breaks do you understand it enough to fix it? If you've never built it then I'm guessing the answer is no. So let's fix that!</p> <p>First we need k0sctl, this will let us deploy the config pretty quickly to our nodes, I also recommend k9s while you're at it</p> <pre><code>go install github.com/k0sproject/k0sctl@latest\ngo install github.com/derailed/k9s@latest\n</code></pre>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#deploying-k0s-via-k0sctl","title":"Deploying k0s via k0sctl","text":"<p>Once you have that you'll need a config file, you can generate one with k0sctl or just use this one:</p> <pre><code>---\napiVersion: k0sctl.k0sproject.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0s-cluster\nspec:\n  hosts:\n    - ssh:\n        address: 2601:123:456:7890::11\n        user: root\n        port: 22\n        keyPath: ~/.ssh/id_ed25519\n      role: controller+worker\n      noTaints: true\n    - ssh:\n        address: 2601:123:456:7890::12\n        user: root\n        port: 22\n        keyPath: ~/.ssh/id_ed25519\n      role: worker\n    - ssh:\n        address: 2601:123:456:7890::13\n        user: root\n        port: 22\n        keyPath: ~/.ssh/id_ed25519\n      role: worker\n  k0s:\n    version: null\n    versionChannel: stable\n    dynamicConfig: false\n    config:\n      apiVersion: k0s.k0sproject.io/v1beta1\n      kind: ClusterConfig\n      metadata:\n        creationTimestamp: null\n        name: k0s\n      spec:\n        network:\n          kubeProxy:\n            mode: ipvs\n            ipvs:\n              strictARP: true\n</code></pre> <p>Since we'll be deploying metallb I've included strictARP for kubeproxy. This config will download the latest version of k0s onto the nodes and get them setup.</p> <p>You can deploy it and update your kube config by doing:</p> <pre><code>mkdir -p ~/.kube\nk0sctl apply --config k0sctl.yaml\nk0sctl kubeconfig --config k0sctl.yaml | tee ~/.kube/config\n</code></pre> <p>I recommend you make an Ansible inventory file to help with deploying and fixing things.</p> <pre><code>---\nall:\n  hosts:\n    controller:\n      ansible_host: 2601:123:456:7890::11\n      ansible_user: root\n    worker1:\n      ansible_host: 2601:123:456:7890::12\n      ansible_user: root\n    worker2:\n      ansible_host: 2601:123:456:7890::13\n      ansible_user: root\n  children:\n    k0s:\n      hosts:\n        controller:\n        worker1:\n        worker2:\n</code></pre>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#fixing-your-messes","title":"Fixing Your Messes","text":"<p>If for someone reason you need to unfuck yourself if (when) you blow it up. Don't worry, it's easy to fix by just going to the nodes and running <code>k0s reset</code> and making sure the <code>/etc/k0s</code> and <code>/var/lib/k0s</code> dirs are removed. Then give it a reboot.</p> <p>You can use this playbook to help automate things:</p> <pre><code>---\n- name: Reset &amp; Remove K0S\n  hosts: k0s\n  tags: [k0s]\n  vars:\n    k0s_services:\n      - k0scontroller\n      - k0sworker\n    k0s_paths:\n      - /var/lib/k0s\n      - /etc/k0s\n  handlers:\n    - name: Reboot\n      ansible.builtin.reboot:\n  tasks:\n    - name: Reset &amp; Remove K0S\n      notify: Reboot\n      ignore_errors: true\n      block:\n        - name: Run Stop Command\n          ansible.builtin.command: \"k0s stop\"\n        - name: Stop K0S Services if still running\n          ansible.builtin.systemd:\n            service: \"{{ item }}\"\n            state: stopped\n          loop: \"{{ k0s_services }}\"\n          register: k0s_stop_results\n          until: k0s_stop_results is success\n          retries: 1\n          delay: 5\n        - name: Run Reset Command\n          ansible.builtin.command: \"k0s reset\"\n        - name: Remove Configs\n          ansible.builtin.file:\n            path: \"{{ item }}\"\n            state: absent\n            force: true\n          loop: \"{{ k0s_paths }}\"\n</code></pre> <p>At this point you have a full cluster ready to go. You can deploy whatever you want to it, use it, abuse it, destroy it, etc.</p>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#helm-charts","title":"Helm Charts","text":"<p>However I'm guessing we'll want to install some helm charts. In this case I'm to be using traefik and metallb to handle my load balancing &amp; cluster ip. I also will be using openebs and nfs for data storage.</p> <p>Just because we're doing this hard way, doesn't mean we don't have to do things the hard way. You can run everything manually however I'm going to use Ansible to add my repos and deploy my charts.</p> <pre><code>---\n- name: Helm Repos, Plugins. &amp; Charts\n  hosts: k0s\n  tags: [k0s]\n  vars:\n    local_user_name: samshamshop\n    nfs_server: nfs.server=nfs.samsfantastichams.org\n    nfs_path: nfs.path=/mnt/nfs/datavol\n  tasks:\n    - name: Add Helm Repos &amp; Plugins\n      become: true\n      become_user: \"{{ local_user_name }}\"\n      delegate_to: localhost\n      run_once: true\n      block:\n        - name: Install Helm env plugin\n          kubernetes.core.helm_plugin:\n            plugin_path: https://github.com/adamreese/helm-env\n            state: present\n        - name: Install Helm diff plugin\n          kubernetes.core.helm_plugin:\n            plugin_path: https://github.com/databus23/helm-diff\n            state: present\n        - name: Add nfs-subdir-external-provisioner repository\n          kubernetes.core.helm_repository:\n            name: nfs-subdir-external-provisioner\n            repo_url: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\n        - name: Add traefik repository\n          kubernetes.core.helm_repository:\n            name: traefik\n            repo_url: https://traefik.github.io/charts\n        - name: Add cert-manager repository\n          kubernetes.core.helm_repository:\n            name: jetstack\n            repo_url: https://charts.jetstack.io\n        - name: Add openebs repository\n          kubernetes.core.helm_repository:\n            name: openebs\n            repo_url: https://openebs.github.io/charts\n        - name: Add longhorn repository\n          kubernetes.core.helm_repository:\n            name: longhorn\n            repo_url: https://charts.longhorn.io\n    - name: Deploy Helm Charts\n      become: true\n      become_user: \"{{ local_user_name }}\"\n      delegate_to: localhost\n      run_once: true\n      block:\n        - name: Deploy cert-manager\n          kubernetes.core.helm:\n            name: cert-manager\n            chart_ref: jetstack/cert-manager\n            release_namespace: cert-manager\n            create_namespace: true\n            set_values:\n              - value: installCRDs=true\n        - name: Deploy MetalLB\n          kubernetes.core.helm:\n            name: metallb\n            chart_ref: metallb/metallb\n            namespace: metallb-system\n            create_namespace: true\n        - name: Deploy traefik\n          kubernetes.core.helm:\n            name: traefik\n            chart_ref: traefik/traefik\n            namespace: traefik\n            create_namespace: true\n        - name: Deploy OpenEBS cStor\n          kubernetes.core.helm:\n            name: openebs\n            chart_ref: openebs/openebs\n            namespace: openebs\n            create_namespace: true\n            set_values:\n              - value: cstor.enabled=true\n        - name: Deploy nfs-subdir-external-provisioner\n          # This fails if run a second time, hence the ignore\n          # https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner\n          kubernetes.core.helm:\n            name: nfs-subdir-external-provisioner\n            chart_ref: nfs-subdir-external-provisioner/nfs-subdir-external-provisioner\n            namespace: kube-system\n            create_namespace: true\n            set_values:\n              - value: \"{{ nfs_server }}\"\n              - value: \"{{ nfs_path }}\"\n          ignore_errors: true\n</code></pre> <p>Give that some time to work though everything and come up. It's going to take a while. You can use <code>k9s</code> to monitor things in the mean time. Once everything is up and running for the most part we can move on.</p>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#openebs-configmap-edits","title":"OpenEBS Configmap Edits","text":"<p>For my purposes openebs does not support lvm out of the box. So we need to tweak the configmap. If you're using LVM or something like that you'll need to do this to make cStor / NDM work for you.</p> <pre><code>EDITOR=vim kubectl edit configmap -n openebs openebs-ndm-config -o yaml\n</code></pre> <p>You'll want to remove <code>/dev/dm-</code> from the path-filter excludes.</p> <pre><code>- key: path-filter\n        name: path filter\n        state: true\n        include: \"\"\n        exclude: \"/dev/loop,/dev/fd0,/dev/sr0,/dev/ram,/dev/md,/dev/rbd,/dev/zd\"\n</code></pre> <p>This will enable you to use lvm volumes. Though if you have attached raw disks, then you really shouldn't need this.</p>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/Kubernetes/#traefik-dashboard-metallb-pools","title":"Traefik Dashboard &amp; Metallb Pools","text":"<p>Now that we have openebs sorted we need to give ourselves someway to access the cluster, since we're on bare metal, let's use metallb and traefik!</p> <pre><code>---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: metal-pool\n  namespace: metallb-system\nspec:\n  addresses:\n    - 10.10.10.20-10.10.10.40\n    - 2601:123:456:7890::1000:1/64\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: metal-l2\n  namespace: metallb-system\n</code></pre> <pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: dashboard\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/dashboard`) || PathPrefix(`/api`)\n      kind: Rule\n      services:\n        - name: api@internal\n          kind: TraefikService  \n</code></pre> <p>We can apply this with kubectl by doing</p> <pre><code>kubectl apply -f metallb-pool.yaml\nkubectl apply -f traefik-dashboard.yaml\n</code></pre> <p>Or we can stick theme of Ansible, because once we have it done in ansible, we can just add more without adding more work.</p> <pre><code>---\n- name: Let's deploy some stuff!\n  hosts: k0s\n  tags: [k0s]\n  vars:\n    local_user_name: samshamshop\n    k8s_dir: \"{{ playbook_dir }}/../k8s\"\n  tasks:\n    - name: Run k8s configs\n      become: true\n      become_user: \"{{ local_user_name }}\"\n      delegate_to: localhost\n      run_once: true\n      block:\n        - name: Install metallb pools\n          kubernetes.core.k8s:\n            src: \"{{ k8s_dir }}/metallb-pool.yaml\"\n            state: present\n            namespace: metallb-system\n        - name: Install traefik dashboard\n          kubernetes.core.k8s:\n            src: \"{{ k8s_dir }}/traefik-dashboard.yaml\"\n            state: present\n            namespace: traefik\n</code></pre> <p>--- WIP ---</p>","tags":["k8s","kubernetes","wtf","wip"]},{"location":"how-to/LXC%20Image%20Creation/","title":"LXC - Image Creation","text":"<p>[!warning]  WARNING: THIS NEEDS SOME LOVE I wrote it back 2019 and I can do this without LXD now NOTE If you want to use LXD this should still work</p>","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/LXC%20Image%20Creation/#building-a-lxc-inside-of-a-lxc","title":"Building a Lxc inside of a Lxc","text":"","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/LXC%20Image%20Creation/#step-0","title":"Step 0","text":"<p>We'll need the following to get started</p> <ul> <li>LXD</li> <li>LXC setup with in LXD</li> <li>Our LXC setup to allow nesting</li> </ul> <p>If you don't have snapd or lxd setup you'll need to do that now</p> <pre><code>sudo yum/apt install -y snap snapd\nsudo snap install lxd\nsudo lxd-init\n</code></pre>","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/LXC%20Image%20Creation/#step-1","title":"Step 1","text":"<p>NOTE: If you want to work with LXD/LXC without sudo then add your self of the LXD group</p> <pre><code>sudo groupadd lxd\nsudo usermod -aG lxd $USER\n</code></pre> <pre><code>sudo lxc launch images:centos/7 NAMEHERE -c security.nesting=true -c security.privileged=true\nsudo lxc exec NAMEHERE -- bash\n</code></pre>","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/LXC%20Image%20Creation/#step-2","title":"Step 2","text":"<pre><code>yum update\nyum install -y epel-release\nyum update\nyum install -y lxc lxc-templates lxc-extra\nlxc-create -n CONTAINERNAMEHERE -t centos\n</code></pre> <p>Now that this is done you'll have a shinny new container!</p> <p>Now let's prepare it!</p>","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/LXC%20Image%20Creation/#step-3","title":"Step 3","text":"<pre><code>cd /var/lib/lxc/CONTAINERNAMEHERE\nchroot rootfs /bin/bash\nyum update\n# ** ANY SPECIAL CONFIGURATIONS SHOULD BE DONE HERE E.G. **\nyum clean all\nyum history new\nfor x in `find /var/log/ -type f` ; do truncate -s 0 $x ; done\nfor x in `find /home/ -type f -name .bash_history` ; do truncate -s 0 $x ; done\nfor x in `find /root/ -type f -name .bash_history` ; do truncate -s 0 $x ; done\nCTRL+D\n</code></pre> <p>Once you have everything installed you'll want to edit the config file however you need it then tar it up.</p> <pre><code>tar --numeric-owner -czvf ../CONTAINERNAMEHERE.tar.gz ./\n</code></pre> <p>This does two things:</p> <ul> <li>It compresses everything including the config</li> <li>two it preserves all the permissions on the rootfs.</li> </ul>","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/LXC%20Image%20Creation/#step-4","title":"Step 4","text":"<p>To restore the container on another system you'll need to make sure that the path exists e.g. <code>mkdir -p /var/lib/lxc/CONTAINERNAMEHERE</code> and place the CONTAINERNAMEHERE.tar.gz into it. Then extract the tarball</p> <pre><code>tar --numeric-owner -xzvf CONTAINERNAMEHERE.tar.gz\nlxc-ls #Should show CONTAINERNAMEHERE\nlxc-start --name CONTAINERNAMEHERE #Should start it\nlxc-info --name CONTAINERNAMEHERE #Should show that it is running\n</code></pre> <p>That's it! Enjoy!</p>","tags":["lxc","lxd","linux","centos"]},{"location":"how-to/Linux/","title":"Linux","text":"","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#error-checking","title":"Error Checking","text":"<p>Systemd failed units</p> <pre><code>systemctl --failed --all\n</code></pre> <p>Error logs</p> <pre><code>journalctl -p 3 -xb\n</code></pre> <p>Broken symlinks</p> <pre><code>find / -xtype l -print\n</code></pre>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#files-open","title":"Files Open","text":"<pre><code>lsof -iTCP -n -P -F pcnfT\n</code></pre>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#strace","title":"STRACE","text":"<p>https://access.redhat.com/solutions/6189481</p> <pre><code>strace -ftttTvyyo /tmp/rpm-strace.out -s 4096 \n</code></pre>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#memory-usage","title":"Memory Usage","text":"<pre><code>ps -e -o pid,vsz,comm= | sort -n -k 2 | tail -n 5\nps aux | head -1; ps aux | sort -rnk 4 | head -5\nps aux | awk '{print $6/1024 \" MB\\t\\t\" $2 \"\\t\" $11}' | sort -n\n</code></pre>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#linux-version-checking","title":"Linux Version Checking","text":"<pre><code>lsb_release\ncat /etc/*release\n</code></pre> <p>RHEL / CENTOS</p> <pre><code>yum install redhat-lsb-core\n</code></pre>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#kswap0-is-eating-my-cpu","title":"KSWAP0 is Eating My CPU","text":"<p>When kswap0 is taking 100% of the CPU and/or you're seeing errors like the following then you're running into a problems that were most likely fixed in a later kernel version.</p> <pre><code>kswapd0: page allocation failure: order:0, mode:0x20\nsystemd-journal: page allocation failure: order:0, mode:0x20\n</code></pre> <p>The gist of what is going on is that you're running out of kernel memory and it can not allocate more pages for itself. This is dead giveaway that your vm.min_free_kbytes value is most likely too low. I've seen this happen most often on boxes that have long uptimes and use services in the kernel space eg. NFS backends.</p> <p> </p> <p>If you look at the error you'll see order:0; that means that the kernel can not allocate more pages for it self. If it were order:1 that would mean that it could not allocate 2 pages, order:4 would be 16 page requests.</p> <p>The mode parameter is a bit field that specifies the type of memory allocation that was requested.\u00a0The value 0x20 indicates that the allocation was made from the kernel\u2019s slab cache.</p> <p>Using a tool like vmstat you can see what the memory limits are and where it's running out of memory. In this case it's most likely expired slabs taking up space that the kernel can't reclaim.</p> <p>To fix this problem upgrading the kernel is normally the best option as this was a known bug in some kernel versions. However in the event this is not possible for whatever reasons run the following commands as they will expand the kernel memory to 1GB, allow the kernel to reclaim memory, and drop the caches. This should stop kswapd from eating the cpu and should allow the kernel to have access to more memory.</p> <p> </p> <p>You may need to change 1 to 2 (or 3) on drop caches to make it more aggressive in freeing up memory. In this case using a value of 3 would make sense due to the request coming from the slab cache. See vm.txt for more information  </p> <pre><code>sysctl vm.min_free_kbytes\nsysctl vm.zone_reclaim_mode\nsudo sed -i '${s/$/'\"\\nvm.min_free_kbytes = 1048576\"'/}' /etc/sysctl.conf\nsudo sed -i '${s/$/'\"\\nvm.zone_reclaim_mode = 1\"'/}' /etc/sysctl.conf\nsudo sysctl -p\necho 1 | sudo tee /proc/sys/vm/drop_caches\n</code></pre> <p> </p> <p>Normally we want vm.zone_reclaim_mode to be set at 0 (the default) for file servers, like NFS, because caching is more important for them. However in this case I set it to 1 so that I can make sure that it's getting enough memory. You'll need to change this on a case by case basis depending on your systems &amp; data. You should consider changing this back to 0 once the system is stable so that services like NFS can take advantage of caching.</p> <p> </p> <p>Linux Kernel Tuning: page allocation failure | Hernan Vivani's Blog</p> <p>Several \"page allocation failure. order:1, mode:0x20\" messages are seen on the console after upgrade to Red Hat Enterprise Linux 6.2 - Red Hat Customer Portal</p> <p>What do the components in a page allocation failure message mean ? - Red Hat Customer Portal</p> <p>kernel.org/doc/Documentation/sysctl/vm.txt</p> <p>Chris's Wiki :: blog/linux/DecodingPageAllocFailures (utoronto.ca)</p> <p>Physical Page Allocation (kernel.org)</p>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#dnf-yum-repo-mirroring-syncing-creation","title":"DNF / YUM Repo Mirroring, Syncing, Creation","text":"<p>Mirroring, Syncing, and Creating repos</p> <p>Yum: sudo yum install yum-utils createrepo</p> <p>DNF: sudo dnf install dnf-utils createrepo</p> <p>To do the initial sync</p> <pre><code>mkdir -p /mnt/your/repo/mirror/here\nreposync --repoid=REPOID --arch=x86_64 --plugins --download_path=/mnt/your/repo/mirror/here\n</code></pre> <p>This will sync whatever repoid (eg epel) you specify, only the x86_64 packages, will allow it to use the yum/dnf plugins, and sync it to your mirror dir. Omitting the repoid flag will sync ALL the repos on the system (/etc/yum.repos.d/)</p> <p>Once the mirror is created you can sync it, verify it, and delete the old packages</p> <pre><code>reposync --repoid=REPOID -arch=x86_64 --plugins --gpgcheck --delete --download_path=/mnt/your/repo/mirror/here\n</code></pre> <p>If you need to do a quick sync you can add <code>--newest-only</code> and it will just sync the newest packages. Adding the <code>--download-metadata</code> flag might be needed for some repos. For example if you want to directly use the repo without running createrepo on it.</p> <p>If you have a collection of RPMs that need to be put together in a repo, update a repo after running reposync, or fix a local mirror with bad metadata, use the <code>createrepo</code> command.</p> <pre><code>createrepo /mnt/your/repo/mirror/here\n</code></pre> <p>If you have a large repo you will want to setup a cache. This will improve your entire repo creation speed at the cost of some disk space.</p> <pre><code>mkdir -p /mnt/your/repo/mirror/here/.repocache\ncreaterepo --update --deltas --cachedir /mnt/your/repo/mirror/here/.repocache /mnt/your/repo/mirror/here\n</code></pre> <p>If you're having problems you can use the <code>--workers</code> flag to set the number workers. By default it will use as many workers as you have threads. Another way of speeding up the process is to remove the <code>--deltas</code> flag to prevent it from generating deltas. It may also be prudent to set nice &amp; ionice levels depending on your system. For non-ssd storage, and/or SAN storage, I typically recommend limiting workers to no more than half of the available threads, nice level of 15, and ionice class of 3. This will make sure the system has plenty of resources to handle large repos without impacting other services. For SSD backed storage, especially local SSDs, ionice typically is not needed unless your storage throughput is limited by something like LUKS.</p> <p> </p> <p>Redhat Monitoring and managing system status and performance</p> <p>PDF - Redhat Monitoring and managing system status and performance.pdf</p>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#security","title":"Security","text":"<p>Harding the boxes</p> <p> </p> <p>PDF - Redhat Enterprise Linux 8 Security Hardening.pdf ## NIST National Checklist for Red Hat Enterprise Linux 8.x</p>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/Linux/#io","title":"IO","text":"<p>https://serverfault.com/questions/169676/how-to-check-disk-i-o-utilization-per-process#</p> <p></p> <p>You can use pidstat to print cumulative io statistics per process every 20 seconds with this command:</p> <pre><code># pidstat -dl 20\n</code></pre> <p>Each row will have following columns:</p> <ul> <li>PID - process ID</li> <li>kB_rd/s - Number of kilobytes the task has caused to be read from disk per second.</li> <li>kB_wr/s - Number of kilobytes the task has caused, or shall cause to be written to disk per second.</li> <li>kB_ccwr/s - Number of kilobytes whose writing to disk has been cancelled by the task. This may occur when the task truncates some dirty pagecache. In this case, some IO which another task has been accounted for will not be happening.</li> <li>Command - The command name of the task.</li> </ul> <p></p> <p>Nothing beats ongoing monitoring, you simply cannot get time-sensitive data back after the event\u2026</p> <p>There are a couple of things you might be able to check to implicate or eliminate however\u2014<code>/proc</code> is your friend.</p> <pre><code>sort -n -k 10 /proc/diskstats\nsort -n -k 11 /proc/diskstats\n</code></pre> <p>Fields 10, 11 are accumulated written sectors, and accumulated time (ms) writing. This will show your hot file-system partitions.</p> <pre><code>cut -d\" \" -f 1,2,42 /proc/[0-9]*/stat | sort -n -k +3\n</code></pre> <p>Those fields are PID, command and cumulative IO-wait ticks. This will show your hot processes, though only if they are still running. (You probably want to ignore your filesystem journalling threads.)</p> <p>The usefulness of the above depends on uptime, the nature of your long running processes, and how your file systems are used.</p> <p>Caveats: does not apply to pre-2.6 kernels, check your documentation if unsure.</p> <p><code>$ sudo iotop -ao</code> # (-a accumulated; -o show only processes with activity)</p> <p>sar -d iostat -y 5 ioping</p> <p>https://www.opsdash.com/blog/disk-monitoring-linux.html</p>","tags":["rocky","rhel","ubuntu","debian","arch"]},{"location":"how-to/MDRAID/","title":"MD Raid Devices","text":"<p>Get Status</p> <pre><code>cat /proc/mdstat # This should give you the status on every box\nmdadm -Esv # Get the overall status and other information about the array\n</code></pre> <p>[!success]</p> <pre><code>root@hostname:~# mdadm -Esv\nARRAY /dev/md/127  level=raid0 metadata=1.2 num-devices=2 UUID=f08bed23:a5b4764a:4a7151ad:65cddd4e name=hostymchostface:127\n   devices=/dev/sdc1,/dev/sdb1\n</code></pre>","tags":["mdraid","storage","raid","linux"]},{"location":"how-to/MDRAID/#troubleshooting","title":"Troubleshooting","text":"<p>Example</p> <pre><code>sudo mdadm -Esv\nsudo mdadm  --stop /dev/md*\nsudo mdadm --misc --scan --detail /dev/md0\nsudo mdadm -v --assemble \"$array\" \"$disk1$part\" \"$disk2$part\"\n</code></pre> <p>If you're getting <code>mdadm: cannot open /dev/sdb1: Device or resource busy</code> or something like that chances are is that the array isn't created correctly. You will need to stop the array with <code>mdadm --stop /dev/md*</code> and recreate it.</p> <p>The rest of the troubleshooting commands are for scanning and re-assembling arrays for example you put the disks in the wrong order. It happens to everyone at least once :)</p>","tags":["mdraid","storage","raid","linux"]},{"location":"how-to/MDRAID/#creating-an-array","title":"Creating an Array","text":"<p>You can make an array a lot of different ways, the current best way is with LVM &amp; DMRAID; however there are A LOT of cases where MDRaid is better to use. A good example is with compatibly with hardware controllers. Dell's PERC orders and creates it's discs in a way that is compatible with importing and exporting via Linux MD devices.</p> <p>Example</p> <pre><code>mdadm --create /dev/md127 --level=stripe --raid-devices=2 --chunk=16 /dev/sdb1 /dev/sdc1\n</code></pre> <p>This example creates a MD device <code>/dev/md127</code> the level is stripe meaning it is striping the data aka RAID0. The <code>raid-devices</code> and <code>chunk</code> are the chunk aka strip size and the number of disks in the array. In this case <code>/dev/sdb1 /dev/sdc1</code> are the two disks in the example. You can configure this as needed</p> <p>Seealso</p> <pre><code>man mdadm\n</code></pre>","tags":["mdraid","storage","raid","linux"]},{"location":"how-to/MDRAID/#rebuilding-an-array","title":"Rebuilding an Array","text":"","tags":["mdraid","storage","raid","linux"]},{"location":"how-to/MTR/","title":"MTR Testing Connections\u2026 for Fun and Profit?","text":"","tags":["mtr","traceroute","networking","openssl","icmp","tcp","udp"]},{"location":"how-to/MTR/#a-word-about-icmp","title":"A Word about ICMP","text":"<p>Keep in mind that doing ICMP traceroutes might show packet loss because ICMP is either turned off or very low priority depending on the carrier, load, and other factors. So while ICMP can give you an idea of where to look it isn't going to give you the whole story.</p>","tags":["mtr","traceroute","networking","openssl","icmp","tcp","udp"]},{"location":"how-to/MTR/#general-testing","title":"General Testing","text":"<p>Example</p> <pre><code>mtr hostname.network\nmtr -T hostname.network\nmtr -U hostname.network\n</code></pre> <p>The first command does ICMP, the second does TCP, the third does UDP.</p> <p>If you need to test a port number add <code>-P</code> with the port number. If you need to make a report to send to someone you can add the <code>-rw</code> flags on to the command. This will make something that you can copy &amp; paste</p>","tags":["mtr","traceroute","networking","openssl","icmp","tcp","udp"]},{"location":"how-to/MTR/#carrier-testing","title":"Carrier Testing","text":"<p>Example</p> <pre><code>mtr -rwnb -z 2 -P 443 -T hostname.network\n</code></pre> <p>This does the following:</p> <ul> <li>Creates a wide report (rw)</li> <li>No DNS lookup &amp; show IPs (nb)</li> <li>Shows Full ASN if possible (-z 2)</li> <li>Traces to port 443 (-P 443)</li> <li>Uses TCP (-T)</li> </ul> <p>This will give you good results that you can use to report to carriers if you're having packet loss problems. It's shows the important bits like ASN &amp; IP addresses. That way they can get their BGP and other data to make sure things are looking correct.</p> <p>Tip</p> <ul> <li> <p>If you're testing and do not need the ASN you can drop the <code>-z 2</code>.</p> </li> </ul> <p>Another helpful flag when generating data is the <code>-c</code>. Using that will set the count aka time. So if you do <code>-c 120</code> that will test for 2 minutes.</p>","tags":["mtr","traceroute","networking","openssl","icmp","tcp","udp"]},{"location":"how-to/MTR/#bonus-trick","title":"Bonus Trick","text":"<p>Use openssl to test your endpoint for life!</p> <p>Example</p> <pre><code>openssl s_client -connect hostname.local:443\n</code></pre> <p>You can test any other port too, eg http, ssh, smtp, mysql, etc. If there's life it will give you something to work with. If you do test a TLS connection with it you will back all the cert data. However this is what it looks like on an active non-tls port</p> <p>Success</p> <pre><code>\u276f openssl s_client -connect duckduckgo.com:80\nCONNECTED(00000005)\n8188707136:error:1404B42E:SSL routines:ST_CONNECT:tlsv1 alert protocol version:/AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/libressl/libressl-3.3/ssl/tls13_lib.c:151:\n--no peer certificate available\n--No client certificate CA names sent\n--SSL handshake has read 5 bytes and written 294 bytes\n--New, (NONE), Cipher is (NONE)\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\nNo ALPN negotiated\nSSL-Session:\n    Protocol  : TLSv1.3\n    Cipher    : 0000\n    Session-ID:\n    Session-ID-ctx:\n    Master-Key:\n    Start Time: 1679671320\n    Timeout   : 7200 (sec)\n    Verify return code: 0 (ok)\n---\n</code></pre>","tags":["mtr","traceroute","networking","openssl","icmp","tcp","udp"]},{"location":"how-to/MySQL/","title":"How to MYSQL","text":"","tags":["mysql","mariadb"]},{"location":"how-to/MySQL/#dump-database-to-a-file","title":"Dump Database to a File","text":"<p>To backup, transfer, etc a database you will need to create a dump. You can dump all the databases or just a single data base. The difference is specifying a database name or the --all-databases flag.</p> <p>Dump a single database  </p> <p><code>mysqldump -u USER -p DatabaseNameHere &gt; DatabaseNameHere-YYYY-MM-DD.sql</code></p> <p>Dump all databases with compression!  </p> <p><code>mysqldump -u USER -p --all-databases | zstd &gt;all-databases-YYYY-MM-DD.sql.zst</code></p> <p>You can also do the same over SSH. You can dump all or just a single database just like above. This is really handy for when you need to transfer a database to another server</p> <p>Dump Over SSH  </p> <p><code>mysqldump -u USER -p DATABASE | ssh USER@HOSTNAME mysql -u USER</code></p>","tags":["mysql","mariadb"]},{"location":"how-to/MySQL/#restoring-a-database","title":"Restoring a Database","text":"<p>You should do the above to dump the database first, just in case. Unless you're REALLY sure of your backups\u2026</p> <p>To restore a database you're going to the same process in reverse. So let's take a compressed dump and put it back into mysql</p> <p>Restoring compressed dump  </p> <p><code>mysql -u USER -p DatabaseNameHere &lt; zstd DatabaseNameHere-YYYY-MM-DD.sql.zst</code></p> <p>Doing the same thing but over ssh  </p> <p><code>zstd DatabaseNameHere-YYYY-MM-DD.sql.zst | ssh USER@HOSTNAME \"mysql -u USER -p DatabaseNameHere\"</code></p> <p>You can mix &amp; match as needed. Just make sure you're backups are good otherwise someone's gonna get hurt real bad\u2026</p>","tags":["mysql","mariadb"]},{"location":"how-to/MySQL/#creating-databases","title":"Creating Databases","text":"<p>Let's say you need to create a database and user to connect to the database, you also need that user to be able to connect from the localhost and other systems.</p> <p>First things first, create a password hash for the user. You can generate it a number of ways, the easiest for this example would be using mysql it self. You can also use python if needed/wanted. Another good resource for generating stuff is RFC Tools</p> <p>With MySQL</p> <pre><code>mysql -NBe \"select password('PasswordHere')\"\n</code></pre> <p>With Python</p> <pre><code>python -c 'from hashlib import sha1; print \"*\" + sha1(sha1(\"PasswordHere\").digest()).hexdigest().upper()'\n</code></pre> <p>Once you have the password hash for the user you want to create, use the following example to create your database</p> <p>Example</p> <pre><code>CREATE DATABASE DatabaseNameHere;\nGRANT ALL PRIVILEGES ON DatabaseNameHere.* to 'UserNameHere'@'%' IDENTIFIED BY PASSWORD '*920018161824B14A1067A69626595E68CB8284CB';\nGRANT ALL PRIVILEGES ON DatabaseNameHere.* to 'UserNameHere'@'localhost' IDENTIFIED BY\nPASSWORD '*920018161824B14A1067A69626595E68CB8284CB';\n</code></pre>","tags":["mysql","mariadb"]},{"location":"how-to/NETWORKING/","title":"NETWORKING","text":"<p>nmcli connection add type dummy ifname dummy0 ipv4.method disabled ipv6.method manual ipv6.addresses fd69:beef:cafe::1/64</p>","tags":["network","networking"]},{"location":"how-to/Podman/","title":"Docker","text":"<p>Docker with MiniKube &amp; Podman</p> <p>https://dhwaneetbhatt.com/blog/run-docker-without-docker-desktop-on-macos</p> <p>[[Run Docker without Docker Desktop on macOS Dhwaneet Bhatt.pdf]]</p>","tags":["podman","docker","lxc","macos"]},{"location":"how-to/Python/","title":"Python Cert Errors","text":"<p>MacOS</p> <pre><code>cd /Applications/Python\\ 3.11/\n./Install\\ Certificates.command\n</code></pre> <p>AND/OR</p> <pre><code>pip install -U certifi\n</code></pre> <p>https://stackoverflow.com/questions/40684543/how-to-make-python-use-ca-certificates-from-mac-os-truststore</p> <p>```python</p>","tags":["python"]},{"location":"how-to/Python/#mostly-derived-from-httpswwwmisterpkicompython-get-ssl-certificate","title":"Mostly derived from https://www.misterpki.com/python-get-ssl-certificate/","text":"<p>class ServerSSLCertificate: \u00a0\u00a0 \"\"\" \u00a0\u00a0 Given a hostname (and optional port), connect to a remote server \u00a0\u00a0 and fetch the server's SSL certificate. This does NOT validate \u00a0\u00a0 the certificate, so can be used on iDRAC and other self-signed \u00a0\u00a0 potentially invalid connections. The resulting certificate is \u00a0\u00a0 loaded and available in PEM form, or as component data in the \u00a0\u00a0 X509 object. \u00a0\u00a0 \"\"\" \u00a0\u00a0 def init(self, server, port=443): \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if not OpenSSL: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 raise RuntimeError(\"ServerSSLCertificate requires OpenSSL module\") \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 connection = ssl.create_connection((server, port)) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 context = ssl.SSLContext() \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sock = context.wrap_socket(connection, server_hostname=server) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.DER = sock.getpeercert(True) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sock.close() \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.PEM = ssl.DER_cert_to_PEM_cert(self.DER) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.X509 = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, self.PEM) ```\u00a0</p>","tags":["python"]},{"location":"how-to/Restic/","title":"Restic","text":"<p>WIP</p>","tags":["restic","backups"]},{"location":"how-to/SELINUX/","title":"SELINUX","text":"<p>[!danger] WORK IN PROGRESS</p> <pre><code>sudo yum install policycoreutils-python-utils\n</code></pre> <pre><code>sudo grep audit.log /var/log/messages | audit2why\nsudo grep audit.log /var/log/messages | audit2allow --module-package=auditdlocal\nsudo semodule --install auditdlocal.pp\n</code></pre> <pre><code>sudo ausearch --raw | audit2why\nsudo ausearch --raw | audit2allow --module-package=auditdlocal\nsudo semodule --install auditdlocal.pp\n</code></pre> <pre><code>auditdlocal.pp\n    &lt;binary&gt;\n</code></pre> <pre><code>auditdlocal.te\n    module auditdlocal 1.0;\n\n    require {\n        type var_log_t;\n        type auditd_t;\n        class file { create open read setattr };\n    };\n\n    allow auditd_t var_log_t:file { create open read setattr };\n</code></pre> <pre><code>sudo checkmodule -M -m -o auditdlocal.mod auditdlocal.te\nsudo semodule_package -o auditdlocal.pp -m auditdlocal.mod\nsudo semodule --install auditdlocal.pp\n</code></pre>","tags":["selinux","linux","security","wip"]},{"location":"how-to/SSH/","title":"SSH","text":"","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#creating-a-ssh-key","title":"Creating a Ssh Key","text":"","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#linux-macos","title":"Linux / MacOS","text":"<p>Example</p> <pre><code>ssh-keygen -t ed25519 -C \"userid and/or email here\"\n</code></pre> <p>Once your key is created you should find it located in <code>~/.ssh</code> using the default options it the path would be <code>~/.ssh/id_ed25519</code> for the private key and <code>~/.ssh/id_ed25519.pub</code> for the public key.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#putty","title":"Putty","text":"<p>Attention:  </p> <p>Make sure you have putty installed for this to work. If you do not have it installed use the package manager of your choice</p> <p>[!example] Using winget to install Putty</p> <pre><code>winget install -e --id PuTTY.PuTTY\n</code></pre> <p>[!example] Creating a new putty key</p> <pre><code>puttygen -t ed25519 -C \"userid and/or email here\" -o putty_key.ppk\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#extracting-keys","title":"Extracting Keys","text":"<p>To use your putty key on without putty you will need to extract the keys. Keep track of where you extra the key too and the file names created, you will need this information later. Also treat these keys like you would a password, they're sensitive information that needs to be protected.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#public-key","title":"Public Key","text":"<p>To get your public key from your putty key you will need to extract it</p> <p>Extracting Public Key</p> <pre><code>puttygen putty_key.ppk -O public-openssh -o putty_key.pub\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#private-key","title":"Private Key","text":"<p>To get your private key from your putty key you will need to extract it</p> <p>Extracting Private Key</p> <pre><code>puttygen putty_key.ppk -O private-openssh -o putty_key\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#convert-openssh-key-to-putty-key","title":"Convert Openssh Key to Putty Key","text":"<p>To convert an openssh key to putty key you will need the private key file as well as any passwords that may or may not be securing that key. This will most likely not apply to anyone unless they want to take an existing key and use in putty</p> <p>Converting the key</p> <pre><code>puttygen id_ed25519 -o putty_key.ppk\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#tpm","title":"TPM","text":"<p>WIP</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#getting-started","title":"Getting Started","text":"<p>Check to see if you have <code>/dev/tpm0</code> on your system. If you do make sure you have tpm2-pkcs11 installed. This is needed to act as a smart card interface. This will allow us to generate and store the private key in the TPM. Making it way more secure than just leaving a private key in your ~/.ssh directory.</p> <p>Next check the group on <code>/dev/tpmrm0</code> and add that group to your user, most likely it will be <code>tss</code> so do <code>sudo usermod -a -G tss \"$(whoami)\"</code> and then relogin.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#create-the-keys","title":"Create the Keys","text":"<pre><code>tpm2_ptool init\ntpm2_ptool addtoken --pid=1 --label=ssh --userpin=PASSWORDHERE --sopin=RECOVERYPASSWORDHERE\ntpm2_ptool addkey --label=ssh --userpin=PASSWORD --algorithm=ecc256\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#reading-the-public-key","title":"Reading the Public Key","text":"<pre><code>ssh-keygen -D /usr/lib/x86_64-linux-gnu/libtpm2_pkcs11.so | tee ~/.ssh/id_tpm.pub\n</code></pre> <p>Take the output from here and add that to the authorized keys file on whatever system you need to connect to.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#using-the-tpm-key","title":"Using the TPM Key","text":"<p>You'll need to point ssh to the identiy file that is the tpm using <code>-I /usr/lib/x86_64-linux-gnu/libtpm2_pkcs11.so</code>. This is easier done in your <code>~/.ssh/config</code> file. To set it for everything add this at the bottom of your config</p> <pre><code> Host *\n   PKCS11Provider /usr/lib/x86_64-linux-gnu/libtpm2_pkcs11.so\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#using-your-ssh-key","title":"Using Your SSH Key","text":"<p>To use your key, simply ssh to a server. It should instantly allow you in without prompting. Putty users will have to enter their user name depending on how they have their connection configured.</p> <p> </p> <p>If you're having problems you may need to add the keys to your ssh-agent. <code>ssh-agent -a ~/.ssh/id_ed25519</code> Putty should do this for you, you may have to go into the connection settings and tell it to use the key. I believe moba works the same way.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#agent-forwarding","title":"Agent Forwarding","text":"<p>For MacOS you will need to do <code>ssh-add --apple-use-keychain</code> to add your keys to the keychain for Agent Forwarding to work correctly</p> <p>When you use agent forwarding it will allow your keys to be used on a remote system that you're connected too. You do this with the <code>-A</code> flag so <code>ssh -A user@host</code> however you can configure your ssh config to do this for you automatically. Make sure you place this at the END of your config</p> <p> </p> <p><code>~/.ssh/config</code></p> <pre><code>host *\n Compression yes\n StrictHostKeyChecking no\n ForwardAgent yes\n</code></pre> <p><code>host *</code> says use these options on anything that matches. This is why we put it at the bottom of <code>~/.ssh/confg</code> because it works on a first match priceable. <code>compression yes</code> means that it will use compression, this is helpful for low-bandwidth connections and data transfers. <code>StrictHostKeyChecking no</code> says don't alert me to accept keys. Not a great idea for most things, but not something we can avoid with our current setup. <code>ForwardAgent yes</code> means that it will forward the ssh-agent to the destination connection. This is the main thing we want, the other options are helpful but not required.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#ssh-agent-forwarding-via-sudo","title":"SSH Agent Forwarding via Sudo","text":"<p>Now that you're forwarding your agent and you're able to ssh into other systems from the system you're connected to, let's assume you need to become root or another user. To do this you will need to forward your shell and environment. Do do this you will do <code>sudo -E -s</code> or <code>sudo -E -s -u username</code>. this will allow you to forward your agent from your system, to a remote system, over sudo, and use your keys that way. The <code>-E</code> flag forwards your environment and the <code>-s</code> forwards your shell.</p> <p>This example forwards your agent, connects to a host, then drops you into root with your agent forwarded</p> <pre><code>ssh -A user@hostname\nsudo -E -s\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#ssh-host-jumping","title":"SSH Host Jumping","text":"<p>To use another host as a jumpbox you have to use the <code>-J</code> flag. You can combine this with the <code>-A</code> to forward your agent too.</p> <pre><code># Use a system to connect to another system\nssh -J user@jump-box user@destination-server\n# Jump from jumpbox to jumpbox2 to jumpbox3 to destination\nssh -J user@jumpbox,user@jumpbox2,user@jumpbox3 user@destination\n</code></pre> <p>If you're forwarding your agent and your public key is on the various servers you can jump to all of them adding the <code>-A</code> flag.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#faster-ssh","title":"Faster SSH","text":"<p>Getting faster ssh connections is simple and easy, you can use the ControlMaster option!</p> <pre><code>host *\n  ControlMaster auto\n  ControlPath ~/.ssh/sockets/%r@%h-%p\n  ControlPersist 600\n</code></pre> <p>This tells ssh to use a socket to reconnect to ssh and hold it open for 600 seconds. This is really helpful if you're bouncing around to different systems and have to reconnect to them. It can cause some problems if you're opening a lot of connections at once. In practice I've only hit a problem a handful of times though and I make A LOT of connections via ssh. So YMMV. If you have problems just remove, comment out, or place it on specific connections rather than all (<code>*</code>) connections</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#ssh-key-signing","title":"SSH Key Signing","text":"","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#signing-data","title":"Signing Data","text":"<p>Now let's take a step further! How about signing files, data, etc? You can do that with your ssh key! Why would you want to do this? You can ensure data integrity and prove that the data came from you.</p> <pre><code>ssh-keygen -Y sign -f ~/.ssh/id_ed25519 -n file FILENAMEHERE\n</code></pre> <p>So lets break this down <code>-Y sign</code> is telling it that you're going to sign something, then comes the path to your private key, next is the name space, since we're signing a file we tell it <code>-n file</code>.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#verifying-data","title":"Verifying Data","text":"<p>Okay, so we signed a file, but how do we verify it?</p> <pre><code>ssh-keygen -Y verify -f allowed_signers -I hugh.smalley@lexisnexisrisk.com -n file -s FILENAMEHERE.sig &lt;FILENAMEHERE\n</code></pre> <p>Just like signing <code>-Y verify</code> is telling it that we want to verify something. <code>-f allow_signers</code> is telling it the name of the file with the information to lookup public keys. Much like the authorized_keys file lets ssh know they's keys are allowed to connect.</p> <p>For example here's my keys in the allowed_signers file.</p> <pre><code>hugh.smalley@lexisnexisrisk.com sk-ssh-ed25519@openssh.com AAAAGnNrLXNzaC1lZDI1NTE5QG9wZW5zc2guY29tAAAAIPEroRTKtLMu9EXxFXXNm8bzA1w/c4v7gFKeFPXmHII1AAAABHNzaDo= Yubikey 5\nhugh.smalley@lexisnexisrisk.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFuAYj8uDz7o5qJzHrEohvfpX/RWBoqudHRoaD+6dR+X Macbook Pro\n</code></pre> <p>The first section is a comma delineated list of identifiers eg email addresses. The next section is the type of key, eg ssh-ed25519, ssh-rsa, etc. Lastly the public key.</p> <p>To continue the <code>-I</code> says the name of the identity to check in the allowed_signers file.<code>-n file</code> is again the name space, <code>-s</code>is the signature file and <code>&lt;FILENAMEHERE</code>is the file we're checking.</p> <p>You can wrap this up in some functions to throw into your .bashrc,.zshrc,etc..</p> <pre><code>ssh_sign () {\n  ssh-keygen -Y sign -f ~/.ssh/id_ed25519 -n file \"${1}\"\n}\n\nssh_verify () {\n  ssh-keygen -Y verify -f allowed_signers -I \"${1}\" -n file -s \"${2}.sig\" &lt;\"${2}\"\n}\n</code></pre>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#code-signing","title":"Code Signing","text":"<p>Now configure git so it knows about your signing key</p> <pre><code>git config --global gpg.format ssh\ngit config --global user.signingkey \"${HOME}/.ssh/id_ed25519.pub\"\n</code></pre> <p>Now when you commit on git add a -S to sign it!</p> <pre><code>git commit -Sam \"Adding, Signing, and Commenting!\"\n</code></pre> <p>If you want to skip adding the S then set:</p> <pre><code>git config --global commit.gpgsign true\n</code></pre> <p>Now you can sign commits and other data with ssh!</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#ssh-certificates","title":"SSH Certificates","text":"<p>WIP</p> <p>A SSH CA is a certificate authority that can issue and sign SSH certificates for authentication. SSH certificates are similar to SSH keys, but they have additional features such as expiration dates and identity information. To create a SSH CA, you need to generate a SSH key pair using the ssh-keygen command and specify a file name and a comment for the CA key\u00b9\u00b2. For example:</p> <pre><code>$ ssh-keygen -t rsa -b 4096 -f host_ca -C host_ca\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in host_ca.\nYour public key has been saved in host_ca.pub.\n</code></pre> <p>This will create a private key file host_ca and a public key file host_ca.pub. The public key file can be used to verify certificates issued by the CA, while the private key file should be kept secure and only used by the CA to sign certificates\u00b9\u00b2.</p> <p>Source: Conversation with Bing, 5/3/2023 (1) How to configure SSH Certificate-Based Authentication - goteleport.com. https://goteleport.com/blog/how-to-configure-ssh-certificate-based-authentication/. (2) 14.3.3. Creating SSH CA Certificate Signing Keys. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/sec-creating_ssh_ca_certificate_signing-keys. (3) GitHub - cloudtools/ssh-ca: Management utilities to support a \u2026. https://github.com/cloudtools/ssh-ca. (4) Using a CA with SSH - lorier.net. https://www.lorier.net/docs/ssh-ca.html. (5) How to configure and setup SSH certificates for SSH authentication. https://dev.to/gvelrajan/how-to-configure-and-setup-ssh-certificates-for-ssh-authentication-b52.</p> <p>To use the CA to sign certificates, you need to have the public keys of the hosts or users that you want to issue certificates for. Then you can use the ssh-keygen command with the -s option to specify the CA private key and the -I option to specify a certificate identity\u00b2\u00b3. For example, to sign a user's public key and create a user certificate, you can use a command like this:</p> <pre><code>$ ssh-keygen -s ca_user_key -I user@example.com id_rsa.pub\n</code></pre> <p>This will create a file named id_rsa-cert.pub that contains the user certificate. The certificate identity can be any alphanumeric value, but it is recommended to use the user name or host name for clarity\u00b2\u00b3. To sign a host's public key and create a host certificate, you need to add the -h option\u00b2\u00b3. For example:</p> <pre><code>$ ssh-keygen -s ca_host_key -I host.example.com -h ssh_host_rsa_key.pub\n</code></pre> <p>This will create a file named ssh_host_rsa_key-cert.pub that contains the host certificate.</p> <p>Source: Conversation with Bing, 5/3/2023 (1) 14.3.3. Creating SSH CA Certificate Signing Keys Red Hat Enterprise \u2026. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/sec-creating_ssh_ca_certificate_signing-keys. (2) How to configure and setup SSH certificates for SSH authentication. https://dev.to/gvelrajan/how-to-configure-and-setup-ssh-certificates-for-ssh-authentication-b52. (3) SSH Best Practices using Certificates, 2FA and Bastions. https://goteleport.com/blog/how-to-ssh-properly/.</p> <p>To configure SSH to trust the CA, you need to copy the CA's public key to the hosts or users that you want to trust the certificates issued by the CA. For user authentication, you need to create a file named /etc/ssh/trusted_user_ca_keys on the SSH server and paste the CA's public key in it\u00b9\u00b3\u2074. For example:</p> <pre><code>$ echo \"cert-authority $(cat host_ca.pub)\" &gt;&gt; /etc/ssh/trusted_user_ca_keys\n</code></pre> <p>This will tell the SSH server to trust any user certificate signed by the CA. For host authentication, you need to create or update a file named /etc/ssh/ssh_known_hosts on the SSH client and paste the CA's public key in it with a @cert-authority prefix\u2075. For example:</p> <pre><code>$ echo \"@cert-authority *.example.com $(cat host_ca.pub)\" &gt;&gt; /etc/ssh/ssh_known_hosts\n</code></pre> <p>This will tell the SSH client to trust any host certificate signed by the CA for any host under the example.com domain.</p> <p>Source: Conversation with Bing, 5/3/2023 (1) Configuring your OpenSSH servers to trust your SSH CA hosted by. https://docs.venafi.com/Docs/22.4/TopNav/Content/SSH/SSHCertificates/t-ssh-certificate-configure-openssh-trust-venafi-ca.php. (2) How to configure SSH Certificate-Based Authentication - goteleport.com. https://goteleport.com/blog/how-to-configure-ssh-certificate-based-authentication/. (3) How to configure and visualize an SSH CA \u00b7 GitHub - Gist. https://gist.github.com/seanw2020/924c50e4c8428ad2d030db99cc819e20. (4) Scalable and secure access with SSH - Engineering at Meta. https://engineering.fb.com/2016/09/12/security/scalable-and-secure-access-with-ssh/. (5) How to configure and setup SSH certificates for SSH authentication. https://dev.to/gvelrajan/how-to-configure-and-setup-ssh-certificates-for-ssh-authentication-b52.</p> <p>To revoke a certificate issued by the CA, you need to add the certificate to a file named /etc/ssh/revoked_keys on the SSH server and specify the file name in the sshd_config file as follows:</p> <pre><code>RevokedKeys /etc/ssh/revoked_keys\n</code></pre> <p>This will tell the SSH server to reject any authentication attempt using the revoked certificate\u00b9. To test if a certificate has been revoked, you can use the ssh-keygen command with the -Qf option to query the revocation list for the presence of the certificate\u00b9. For example:</p> <pre><code>$ ssh-keygen -Qf /etc/ssh/revoked_keys id_rsa-cert.pub\n</code></pre> <p>This will return either id_rsa-cert.pub: revoked or id_rsa-cert.pub: valid depending on the status of the certificate. Alternatively, you can revoke a CA certificate by changing the cert-authority directive to revoke in the known_hosts file\u00b9. For example:</p> <pre><code>@revoke *.example.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC0g+ZTxC7weoIJLUafOgrm+h\u2026\n</code></pre> <p>This will tell the SSH client to reject any host certificate signed by the revoked CA for any host under the example.com domain.</p> <p>Source: Conversation with Bing, 5/3/2023 (1) 14.3.8. Revoking an SSH CA Certificate - Red Hat Customer Portal. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/sec-revoking_an_ssh_ca_certificate. (2) How to Revoke SSH Certificates - CybersecurityHQ.io. https://www.cybersecurityhq.io/blog/how-to-revoke-ssh-certificates. (3) Hashicorp Vault revoke SSH signed certificate - Stack Overflow. https://stackoverflow.com/questions/68812537/hashicorp-vault-revoke-ssh-signed-certificate.</p> <p>You can specify additional options for the certificates when you sign them using the ssh-keygen command. Some of the options are:</p> <ul> <li>-V to specify the validity interval of the certificate. For example, -V +1h means the certificate is valid for one hour from now\u00b2\u2074.</li> <li>-n to specify the principals (user names or host names) that are allowed to use the certificate. For example, -n darren means only the user named darren can use the certificate\u00b2\u2074.</li> <li>-z to specify a serial number for the certificate. For example, -z 1 means the certificate has a serial number of 1\u00b2\u2074.</li> <li>-O to specify one or more certificate options that can restrict or permit certain features. For example, -O no-agent-forwarding means the certificate cannot be used to forward an authentication agent\u00b2\u00b3.</li> </ul> <p>For example, to sign a user's public key and create a user certificate that is valid for one hour, allows only the user named darren to use it, has a serial number of 1, and does not allow agent forwarding, you can use a command like this:</p> <pre><code>$ ssh-keygen -s ca_user_key -I darren -n darren -V +1h -z 1 -O no-agent-forwarding id_rsa.pub\n</code></pre> <p>This will create a file named id_rsa-cert.pub that contains the user certificate with the specified options\u2074.</p> <p>Source: Conversation with Bing, 5/3/2023 (1) How to configure and setup SSH certificates for SSH authentication. https://dev.to/gvelrajan/how-to-configure-and-setup-ssh-certificates-for-ssh-authentication-b52. (2) How to configure SSH Certificates And User Principals - CottonLinux. https://cottonlinux.com/ssh-certificates/. (3) 14.3.5. Creating SSH Certificates - Red Hat Customer Portal. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/sec-signing_ssh_certificates. (4) How do I specify the key exchange method in OpenSSH?. https://superuser.com/questions/744816/how-do-i-specify-the-key-exchange-method-in-openssh. (5) Get self signed certificate of remote server - Stack Overflow. https://stackoverflow.com/questions/9072376/configure-git-to-accept-a-particular-self-signed-server-certificate-for-a-partic. (6) How to configure SSH Certificate-Based Authentication - goteleport.com. https://goteleport.com/blog/how-to-configure-ssh-certificate-based-authentication/.</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#certificate-authority","title":"Certificate Authority","text":"<p>WIP</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#host-certificates","title":"Host Certificates","text":"<p>WIP</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#client-certificates","title":"Client Certificates","text":"<p>WIP</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#congrats","title":"Congrats","text":"<p> you did it!</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SSH/#see-also","title":"See Also","text":"<p>Github has a guide for creating, using, and adding ssh keys. I suggest you read that if you want to know more or add your keys to github. Ledger's guide to TPM2 on Debian is really good and provided a lot of information on how to do TPM ssh Pandasauce guide on smart cards</p>","tags":["ssh","putty","winget","git"]},{"location":"how-to/SVN/","title":"SVN","text":"<p>Make sure your EDITORs are setup, if in doubt</p> <pre><code>export EDITOR=vim; export SVN_EDITOR=vim\n</code></pre>","tags":["svn"]},{"location":"how-to/SVN/#common-svn-commands","title":"Common SVN Commands","text":"","tags":["svn"]},{"location":"how-to/SVN/#checking-out-a-repo","title":"Checking out a Repo","text":"<p>Example</p> <pre><code>svn co https://sams-fantasic-hams.io/repos/scripts\n</code></pre> <p>This will checkout the scripts directory the repos.</p>","tags":["svn"]},{"location":"how-to/SVN/#checking-the-status","title":"Checking the Status","text":"<p>Example</p> <pre><code>svn info\nsvn status\nsvn log\nsvn log --diff\n</code></pre> <p>With the above commands it will assume your current working directory as the path. You can use a file path such as <code>~/svn/scripts</code> you can also a URI like <code>file:///home/user/svn/scripts/file_or_dir_here</code> or <code>https://sams-fantasic-hams.io/repos/scripts/file_or_dir_here</code></p> <p>When using <code>svn log</code> commands you should pipe it into <code>less</code> or <code>head</code> otherwise there will be a lot of information in your terminal</p>","tags":["svn"]},{"location":"how-to/SVN/#committing-changes","title":"Committing Changes","text":"<p>Example</p> <pre><code>svn commit -m \"Commit Message Here\"\n</code></pre>","tags":["svn"]},{"location":"how-to/SVN/#adding-a-new-file","title":"Adding a New File","text":"<p>Example</p> <pre><code>svn add NewFileHere\n</code></pre>","tags":["svn"]},{"location":"how-to/SVN/#creating-a-new-directory","title":"Creating a New Directory","text":"<p>Example</p> <pre><code>svn mkdir NewDirHere\n</code></pre>","tags":["svn"]},{"location":"how-to/SVN/#removing-a-data","title":"Removing a Data","text":"<p>Example</p> <pre><code>svn rm DataToRemoveHere\n</code></pre>","tags":["svn"]},{"location":"how-to/SVN/#reverting-changes","title":"Reverting Changes","text":"<p>Example</p> <pre><code>svn merge -r current_version:previous_version\n</code></pre> <p>There is a shortcut to just revert the latest change <code>svn merge -c -REV .</code> or if you need to do multiple commits add them with a comma <code>svn merge -c -REV,-REV .</code></p> <p>Once you do that, you will then need to commit the changes. <code>svn commit -m \"Reverted to REV\"</code></p>","tags":["svn"]},{"location":"how-to/SVN/#accept-server-cert","title":"Accept Server Cert","text":"<pre><code>svn --trust-server-cert \nsvn --trust-server-cert --non-interactive\n</code></pre>","tags":["svn"]},{"location":"how-to/Steam/","title":"Steam","text":"","tags":["steam","linux","gaming"]},{"location":"how-to/Steam/#speed-up-shader-pre-caching","title":"Speed Up Shader Pre-Caching","text":"<p>Flatpak:</p> <pre><code>printf \"%s\" \"unShaderBackgroundProcessingThreads $(nproc)\" | tee $HOME/.var/app/com.valvesoftware.Steam/data/Steam/steam_dev.cfg\n</code></pre> <p>Distro:</p> <pre><code>printf \"%s\" \"unShaderBackgroundProcessingThreads $(nproc)\" | tee $HOME/.local/share/Steam/steam_dev.cfg\n</code></pre> <p> </p> <p>https://youtu.be/F-ffYldeJaE</p>","tags":["steam","linux","gaming"]},{"location":"how-to/Steam/#looking-glass","title":"Looking Glass","text":"<p>https://blandmanstudios.medium.com/tutorial-the-ultimate-linux-laptop-for-pc-gamers-feat-kvm-and-vfio-dee521850385 https://passthroughpo.st/simple-per-vm-libvirt-hooks-with-the-vfio-tools-hook-helper/ https://github.com/PassthroughPOST/VFIO-Tools/tree/master/libvirt_hooks https://github.com/joeknock90/Single-GPU-Passthrough/tree/master</p>","tags":["steam","linux","gaming"]},{"location":"how-to/TAR/","title":"How I Learned to Love <code>tar</code>","text":"<p>Tar stands for tape archive. So if you think of tar files like tape archives it makes it a lot easier to manage. Or so I think because I worked with tapes for years.</p>","tags":["tar"]},{"location":"how-to/TAR/#create-an-archive","title":"Create an Archive","text":"<p>Creating an archive is as simple as <code>-cf</code> the c is create and f is for file then your data paths</p> <p>Example</p> <pre><code>tar -cf file.name.tar Dir.To.Tar or.Files.too\n</code></pre>","tags":["tar"]},{"location":"how-to/TAR/#compression-decompression","title":"Compression &amp; Decompression","text":"<p>Tar has an automatic mode for a while now, as long as you a way to do it, tar will figure it out most of the time for you. Let's compress something with the new hotness of zstd. In this example <code>-avcf</code> stands for automatic, verbose, create, file</p> <p>Example</p> <pre><code>tar -avcf file.name.tar.zst Dir.To.Tar and.or.Files.too\n</code></pre> <p>Now how about extracting a file?</p> <p>Example</p> <pre><code>tar -avxf file.name.tar.gz\n# or to somewhere else\ntar -avxf file.name.tar.gz -C /your/path/here\n</code></pre> <p>In that example we told tar to use the flags <code>-avxf</code> that stands for automatic, verbose, extract, file.</p>","tags":["tar"]},{"location":"how-to/TAR/#using-custom-compression","title":"Using Custom Compression","text":"<p>If you need to use something that isn't built into tar, for instance if you're using a version of tar that does not support zstd you can use <code>-I</code> to set the program to use for compression</p> <p>Example</p> <pre><code>tar -I zstd -cf filename.tar.zst /your/path/here\n</code></pre> <p>Note</p> <ul> <li> <p>Keep in mind, although this doesn\u2019t apply to tar itself, when dealing with most compression programs there is a big difference between <code>-e</code> and <code>-x</code> being that <code>-e</code> will preserve paths.</p> <p>So if Alice made a tar file in <code>/home/alice/mytardis</code> that means that when Bob uses <code>-e</code> it will try to extract the data to the same path. So if Bob wants <code>/home/bob/mytardis</code> rather than <code>/home/alice/tardis</code> Bob needs to use the <code>-x</code> flag.</p> </li> </ul>","tags":["tar"]},{"location":"how-to/TAR/#adding-and-removing-data","title":"Adding and Removing Data","text":"<p>So you want to add or remove data from your tar file? Good news, you can! </p> <p>However it's not as easy as you may like it to be. In most causes it might be a lot easier to use the <code>--exclude</code> flag and remake the file. However let's solve this problem one step at a time.</p> <p> </p> <p>Did you know the <code>--exlude</code> flag also works for extracting?</p> <p>First things first, let's get a list of all the file in the tar. Here are two examples on how to list things and search things in a tar file.</p> <p>Example</p> <pre><code>tar -atf file.name.here.tar.bz2\ntar -atf file.name.here.tar.zst --wildcards '*.png'\n</code></pre> <p>Tip</p> <ul> <li> <p>Don't worry if you leave off the <code>-a</code> as tar is pretty smart and will figure out that it's compressed. Just remember to use the main part of the command e.g. <code>-tf</code> at min.</p> </li> </ul> <p>Now that you know the path in the file to the data you're going to use the <code>--delete</code> flag to remove the data or the <code>--append</code> flag to append data.</p> <p> </p> <p>Keep in mind, there is no way to append or delete from a compressed archive.</p> <p>If you have a compressed tar you will need to decompress it, let's use <code>file.name.here.tar.zstd</code> as an example. This works the same for anything the file was compressed with e.g. <code>zstd</code> or <code>gzip</code>.</p> <p>Example</p> <pre><code>zstd -d file.name.here.tar.zstd\ntar -vf file.name.here.tar --delete path/here/oops.foo\ntar -vf file.name.here.tar --append path/here/yes_this.bar\nzstd file.name.here.tar -o file.name.here.tar.zstd\n</code></pre> <p>So what we did is decompress <code>file.name.here.tar.zstd</code> and that became <code>file.name.here.tar</code>. Now we removed <code>oops.foo</code> and added <code>yes_this.bar</code> to the tar file. Lastly we compressed <code>file.name.here.tar</code> to create a shiny new <code>file.name.here.tar.zstd</code>.</p>","tags":["tar"]},{"location":"how-to/TAR/#backups-with-tar","title":"Backups with Tar","text":"<p>Tar is or was a backup tool first and for most, so it goes without saying you can do the normal differential and incremental backups with it. Differential backups are hard to automate so let's just stick with incremental backups for now.</p>","tags":["tar"]},{"location":"how-to/TAR/#give-it-the-g","title":"Give it the <code>-g</code>","text":"<p>When you want to create an incremental backup you will need to first create a full backup and a snapshot file.</p> <p>Example</p> <pre><code>tar -I zstd -g /path/to/backup/backup.snar -cf /path/to/backup/full_backup.tar.zst /path/to/data/here\n</code></pre> <p>This command will compress the backup using zstd, create the snar file, and make a fill backup. Once that's done you can make as many backups you want and the snar will store lists of changes between backups.</p> <p>Now to make our first incremental backup!</p> <p>Example</p> <pre><code>tar -I zstd -g /path/to/backup/backup.snar -cf /path/to/backup/$(date '+%F').tar.zst /path/to/data/here\n</code></pre>","tags":["tar"]},{"location":"how-to/TAR/#excluding-data","title":"Excluding Data","text":"<p>Tar can exclude data a number of ways</p>","tags":["tar"]},{"location":"how-to/TAR/#file-steaming-with-tar","title":"File Steaming with Tar","text":"<p>Tar was created to stream data to tapes. You can do a lot of things that you wouldn't think you should be able to do with it. However thanks to the Unix way\u2026 EVERYTHING is a file.</p>","tags":["tar"]},{"location":"how-to/TAR/#copying-a-large-amount-of-small-files","title":"Copying a Large Amount of Small Files","text":"<p>Let's say we have 10 million small files that we need to move from <code>/var/omg/why/did/you/do/this</code> to <code>/mnt/this/is/where/it/should/have/been</code> that is going to take a while right? Well\u2026.</p> <p>Example</p> <pre><code>tar -cf - /var/omg/why/did/you/do/this | tar -xf - -C /mnt/this/is/where/it/should/have/been\n</code></pre> <p>You can also do this over ssh! Now let's add some compression too!</p> <p>Example</p> <pre><code># Make a file on the other end\ntar -cf - /var/lib/data | zstd | ssh user@hostname \"cat &gt;/var/backups/backup.tar.zst\"\n# Send the data compressed with a progress bar!\ntar -cf - /var/lib/data | pv | zstd | ssh user@hostname tar -axf - -C /var/lib/data/\n# Send the data with a progress bar\ntar -cf - /var/lib/data | pv | ssh user@hostname tar -xf - -C /var/lib/data/\n# Get the data\nssh user@hostname \"tar -cf - /var/lib/data\" | tar -xvf - -C /var/lib/data/\n</code></pre> <p>Note</p> <ul> <li> <p>The pipe view command <code>pv</code> will give you different information depending on where it's put and what flags are used. The example above shows the raw data being passed. If you wanted to see the compressed data passed then you should put it after <code>zstd</code></p> </li> </ul> <p>The above examples show how to use tar with ssh. You can mix and match as needed. Your only limit is your creativity!</p> <p>Oh and that being said, <code>rsync</code> is almost always the better tool for sending data around. The edge cases being special files like symbolic links, special devices, sockets, named pipes, etc. If you're sending and backing up anything like that or other data rsync is bad for, tar is your go too.</p>","tags":["tar"]},{"location":"how-to/TLS/","title":"TLS Cert Creation","text":"","tags":["tls"]},{"location":"how-to/TLS/#create-a-csr","title":"Create a CSR","text":"<pre><code>openssl genrsa -out cert_name.key 2048\nopenssl req -config openssl.cnf -new -key cert_name.key -out cert_name.csr\n</code></pre>","tags":["tls"]},{"location":"how-to/TLS/#check-contents-of-csr","title":"Check Contents of CSR","text":"<pre><code>openssl req -in cert_name.csr -noout -text\n</code></pre>","tags":["tls"]},{"location":"how-to/TLS/#create-certificate","title":"Create Certificate","text":"<pre><code>openssl ca -config openssl.cnf -extensions server_cert -in cert_name.csr -out cert_name.pem\n</code></pre>","tags":["tls"]},{"location":"how-to/TLS/#verify-certificate","title":"Verify Certificate","text":"<pre><code>openssl x509 -noout -text -in cert_name.pem\n</code></pre>","tags":["tls"]},{"location":"how-to/TLS/#verify-certificate-and-key-matches","title":"Verify Certificate and Key Matches","text":"<pre><code>$ openssl x509 -noout -modulus -in cert_name.pem | openssl md5\n(stdin)=8298db7d4e44ec7c0ca11a831edcb01d\n\n$ openssl rsa -noout -modulus -in cert_name.key | openssl md5\n(stdin)=8298db7d4e44ec7c0ca11a831edcb01d\n</code></pre> <p>https://jamielinux.com/docs/openssl-certificate-authority/sign-server-and-client-certificates.html</p>","tags":["tls"]},{"location":"how-to/TMUX/","title":"TMUX - Let's Multitask","text":"","tags":["tmux","shell","bash","ssh","terminal"]},{"location":"how-to/TMUX/#getting-started","title":"Getting Started","text":"<p>Before doing anything I recommend you getting oh-my-tmux.</p> <pre><code>cd ~\ngit clone https://github.com/gpakosz/.tmux.git\nln -s -f .tmux/.tmux.conf\ncp .tmux/.tmux.conf.local .\n</code></pre> <p>However if you can not for whatever reason vanilla tmux and oh-my-tmux share almost all the same keybindings.</p>","tags":["tmux","shell","bash","ssh","terminal"]},{"location":"how-to/TMUX/#starting-tmux","title":"Starting Tmux","text":"<p>New Session</p> <pre><code>tmux\ntmux new -s myname # New Named Session\n</code></pre> <p>Joining a Session</p> <pre><code>tmux ls # List the sessions\ntmux -a X # X is the session number\ntmux -a -t myname # Join a session by name\n</code></pre> <p>Ending a Session</p> <pre><code>tmux kill-session -t myname\n</code></pre>","tags":["tmux","shell","bash","ssh","terminal"]},{"location":"how-to/TMUX/#using-tmux","title":"Using TMUX","text":"<p>In tmux, you have to prefix <code>ctrl+b</code> each command you do. That way it knows when you're giving tmux a command vs just typing out something.</p>","tags":["tmux","shell","bash","ssh","terminal"]},{"location":"how-to/TMUX/#session-management","title":"Session Management","text":"<pre><code>:new&lt;CR&gt; new session\ns list sessions\n$ name session\n\n## Windows (tabs)\n\nc           new window\n,           name window\nw           list windows\nf           find window\n&amp;           kill window\n.           move window - prompted for a new number\n:movew&lt;CR&gt;  move window to the next unused number\n\n## Panes (splits)\n\n%  horizontal split\n\"  vertical split\n\no  swap panes\nq  show pane numbers\nx  kill pane\n\u237d  space - toggle between layouts\n\n## Window/pane Surgery\n\n:joinp -s :2&lt;CR&gt;  move window 2 into a new pane in the current window\n:joinp -t :1&lt;CR&gt;  move the current pane into a new pane in window 1\n\n## Misc\n\nd  detach\nt  big clock\n?  list shortcuts\n:  prompt\n</code></pre> <p>Seealso</p> <ul> <li>Move window to pane</li> <li>How to reorder windows</li> </ul>","tags":["tmux","shell","bash","ssh","terminal"]},{"location":"how-to/TMUX/#oops-i-need-to-disconnect","title":"Oops I Need to Disconnect","text":"<p>This works, most of the time:</p> <p>Prerequisites: have\u00a0<code>reptyr</code>\u00a0and\u00a0<code>tmux</code>/<code>screen</code>\u00a0installed; you'll be able to find them with\u00a0<code>apt-get</code>\u00a0or\u00a0<code>yum</code>, depending on your platform.</p> <ol> <li> <p>Use\u00a0Ctrl+Z\u00a0to suspend the process.</p> </li> <li> <p>Resume the process in the background with\u00a0<code>bg</code></p> </li> <li> <p>Find the process ID of the background process with\u00a0<code>jobs -l</code></p> <p>You'll see something similar to this:</p> <pre><code>[1]+ 11475 Stopped (signal) yourprocessname\n</code></pre> </li> <li> <p>Disown the job from the current parent (shell) with\u00a0<code>disown yourprocessname</code></p> </li> <li> <p>Start\u00a0<code>tmux</code>\u00a0(preferred), or\u00a0<code>screen</code>.</p> </li> <li> <p>Reattach the process to the\u00a0<code>tmux</code>/<code>screen</code>\u00a0session with reptyr:</p> <pre><code>reptyr 11475\n</code></pre> </li> <li> <p>Now you can detach the multiplexer (default\u00a0Ctrl+B,\u00a0D\u00a0for\u00a0<code>tmux</code>, or\u00a0Ctrl+A,\u00a0D\u00a0for\u00a0<code>screen</code>), and disconnect SSH while your process continues in\u00a0<code>tmux</code>/<code>screen</code>.</p> </li> <li> <p>Later when you connect with SSH again, you can then attach to your multiplexer (e.g.\u00a0<code>tmux attach</code>).</p> </li> </ol> <p>Share</p>","tags":["tmux","shell","bash","ssh","terminal"]},{"location":"how-to/Taskfile/","title":"Taskfile - The Modern Makefile","text":"<p>Task is a modern version of make written in go. They allow you to use yaml &amp; jinja to define your actions and have the system build things for you. I use them to build an OPs environment and do on boarding. It's a quick and easy way to make sure you have the tools needed to do your job.</p> <p>Example</p> <pre><code>version: \"3\"\n\ntasks:\n  hello:\n    cmds:\n      - echo 'Hello World from Task!'\n    silent: true\n</code></pre> <p>Here's an example of getting a HPC python and ansible ops environment going</p> <p>Example</p> <pre><code>---\nversion: \"3\"\nvars:\ntasks:\n  default:\n    desc: Install Ops stuff\n    depends:\n      - pips_needed\n      - radssh_install\n      - galaxy_needed\n pips_needed:\n   desc: Installs python packages needed\n   dir: \"{{.USER_WORKING_DIR}}\"\n   cmds: [pip install -U --user -r requirements.txt]\n radssh_install:\n   desc: Installs Radssh\n   cmds:\n     - pip install -U --user paramiko\n     - pip install -U --user --use-pep517 git+https://github.com/radssh/radssh.git\n galaxy_needed:\n   desc: Installs galaxy collections and roles\n   dir: \"{{.USER_WORKING_DIR}}\"\n   cmds: [ansible-galaxy install -r requirements.yml]\n</code></pre> <p>While it may look complicated, it's pretty simple. From the top to the bottom it says. When run <code>go-task Taskfile.yml</code> install the default task. The default task depends on the following tasks, pips_needed, radssh_install, galaxy_needed. So it will run those in order to make sure it has everything it needs. The pips_needed task says use the current working directory and run these commands. In this case it's just a single command to pip install the requirements.txt file. The next section uses pip to make sure that paramiko and radssh is installed from github. Lastly the final task, galaxy_needed does the same thing as the first task but with ansible-galaxy rather than pip.</p> <p>I have Taskfiles created for all of my most commonly used systems from RHEL to MacOS and everything in-between. If you need or want to use them let me know!</p> <p>Seealso</p> <ul> <li>Taskfile Usage</li> <li>Taskfile API</li> <li>Taskfile Styleguide</li> </ul>","tags":["taskfile","terminal"]},{"location":"how-to/VIM/","title":"VIM (and VI)","text":"<p>Quit VI and VIM  </p> <p><code>:q</code> - Quit <code>:q!</code> - Force Quit, DO NOT SAVE <code>:wq</code> - Write (Save) Changes &amp; Quit <code>:wq!</code> - Write Changes &amp; Force Quit</p> <p>Paste Mode aka stop trying to be helpful and format things for me  </p> <p><code>:set paste</code></p> <p>Search and Replace  </p> <p><code>:%s/STRING_OR_REGEX/REPLACE_WITH/g</code> &gt; <code>:%s/BADSTRING/GOODSTRING/g</code> &gt; <code>:%s/192\\.168\\.100\\.100/10.100.100.100/g</code></p> <p>Remove Trailing Whitespaces  </p> <p><code>:%s/\\s\\+$//</code></p> <p>Macro Mode  </p> <p><code>qq</code></p>","tags":["vim","vi"]},{"location":"how-to/Warewulf/","title":"Warewulf","text":"<p>WIP</p>","tags":["hpc","cluster","warewulf"]},{"location":"how-to/Windows/","title":"Windows","text":"","tags":["windows","powershell","winget","chocolatey"]},{"location":"how-to/Windows/#setup-script","title":"Setup Script","text":"<p>Here's a really good looking setup script that's actively maintained</p> <pre><code>PowerShell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://gist.githubusercontent.com/mikepruett3/7ca6518051383ee14f9cf8ae63ba18a7/raw/shell-setup.ps1'))\"\n</code></pre> <p>[[shell-setup.ps1]]</p>","tags":["windows","powershell","winget","chocolatey"]},{"location":"how-to/macOS/","title":"MacOS","text":"","tags":["macos","apple","osx","sudo","sed"]},{"location":"how-to/macOS/#sudo-w-touch-id","title":"Sudo w/ Touch ID","text":"<p>Add the following to <code>/etc/pam.d/sudo</code></p> <pre><code>auth       sufficient     pam_tid.so\n</code></pre>","tags":["macos","apple","osx","sudo","sed"]},{"location":"how-to/macOS/#problems-with-sed","title":"Problems with Sed","text":"<p>If you're getting problems using <code>sed -i</code> that is because macOS/OSX version of <code>sed</code> expects an argument after <code>-i</code>, the suffix to use for the backup files. This is not the same with GNU <code>sed</code>.</p> <pre><code>sed: 1: \"./.DS_Store\": invalid command code .\n</code></pre> <p>So to work around this do <code>-i \"\"</code> or <code>-i .bak</code>.</p>","tags":["macos","apple","osx","sudo","sed"]},{"location":"how-to/macOS/#tips","title":"Tips","text":"<p>Show hidden files - Command + Shift + . (period)</p>","tags":["macos","apple","osx","sudo","sed"]},{"location":"recipes/Marry%20Me%20Chicken/","title":"Marry Me Chicken","text":"","tags":["recipes","chicken","reddit"]},{"location":"recipes/Marry%20Me%20Chicken/#ingredients","title":"Ingredients","text":"<p>3 - Chicken Breasts, slic 100g - flour 10 - cherry tomatoes, sliced in half 150g - sun dried tomatoes 2tbsp - olive oil (or oil from sun dried tomatoes) 4 - finely diced garlic cloves 200ml - stock (chicken or veg) 300ml - heavy cream (or thick coconut milk) 30g - grated parm +10g for garnish 1 tsp - chili flakes (optional) 1 tsp - tomato paste 1tbsp - oregano fresh basil for garnish s&amp;p to taste</p>","tags":["recipes","chicken","reddit"]},{"location":"recipes/Marry%20Me%20Chicken/#prepare","title":"Prepare","text":"<ol> <li>Preheat oven to 140c and place sliced cherry tomatoes on and drizel with oil and s&amp;p for 45min</li> <li>Season chicken with s&amp;p and coat with flour</li> <li>Preheat frying pan with olive oil and butter and brown chicken, set aside when done</li> <li>Saute garlic, add stock, next add cream &amp; cheese. Simmer for a few min and add tomato paste and spices.</li> <li>Add chopped sun dried tomotoes and chicken back into the pot and simmer until cooked</li> <li>Garnish with cherry tomotoes, basil, parsley and extra parm. Serve over pasta/rice/pearl barley.</li> <li>Enjoy</li> </ol>","tags":["recipes","chicken","reddit"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#ad","title":"ad","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#afp","title":"afp","text":"<ul> <li>AFP</li> </ul>"},{"location":"tags/#afpfs","title":"afpfs","text":"<ul> <li>AFP</li> </ul>"},{"location":"tags/#airprint","title":"airprint","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#ansible","title":"ansible","text":"<ul> <li>Ansible</li> </ul>"},{"location":"tags/#apple","title":"apple","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#appletalk","title":"appletalk","text":"<ul> <li>AFP</li> </ul>"},{"location":"tags/#apptainer","title":"apptainer","text":"<ul> <li>Apptainer</li> <li>HPC</li> </ul>"},{"location":"tags/#arch","title":"arch","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#audio","title":"audio","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#backups","title":"backups","text":"<ul> <li>Restic</li> </ul>"},{"location":"tags/#bash","title":"bash","text":"<ul> <li>Bash</li> <li>TMUX</li> </ul>"},{"location":"tags/#centos","title":"centos","text":"<ul> <li>LXC - Image Creation</li> </ul>"},{"location":"tags/#chicken","title":"chicken","text":"<ul> <li>Marry Me Chicken</li> </ul>"},{"location":"tags/#chocolatey","title":"chocolatey","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#cluster","title":"cluster","text":"<ul> <li>Apptainer</li> <li>HPC</li> <li>Warewulf</li> </ul>"},{"location":"tags/#containers","title":"containers","text":"<ul> <li>LXC</li> </ul>"},{"location":"tags/#cups","title":"cups","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#debian","title":"debian","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#dns","title":"dns","text":"<ul> <li>DNS</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>DOCKER</li> <li>Containers</li> </ul>"},{"location":"tags/#domain","title":"domain","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#dotfiles","title":"dotfiles","text":"<ul> <li>dotfiles</li> </ul>"},{"location":"tags/#ffmpeg","title":"ffmpeg","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#freeipa","title":"freeipa","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#fresh","title":"fresh","text":"<ul> <li>dotfiles</li> </ul>"},{"location":"tags/#frontmatter","title":"frontmatter","text":"<ul> <li>Markdown Metadata, its meta-tastic!</li> </ul>"},{"location":"tags/#gaming","title":"gaming","text":"<ul> <li>STEAM</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>GIT</li> <li>SSH</li> </ul>"},{"location":"tags/#hpc","title":"hpc","text":"<ul> <li>Apptainer</li> <li>HPC</li> <li>Warewulf</li> </ul>"},{"location":"tags/#icmp","title":"icmp","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#idm","title":"idm","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#ip6","title":"ip6","text":"<ul> <li>IPv6</li> </ul>"},{"location":"tags/#ipa","title":"ipa","text":"<ul> <li>IPA</li> </ul>"},{"location":"tags/#ipfs","title":"ipfs","text":"<ul> <li>This is My Second IPFS Post</li> <li>This is My First IPFS Post</li> </ul>"},{"location":"tags/#ipp","title":"ipp","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#ipv6","title":"ipv6","text":"<ul> <li>IPv6</li> </ul>"},{"location":"tags/#json","title":"json","text":"<ul> <li>JSON</li> </ul>"},{"location":"tags/#k8s","title":"k8s","text":"<ul> <li>K8S</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>K8S</li> </ul>"},{"location":"tags/#linux","title":"linux","text":"<ul> <li>LXC - Image Creation</li> <li>MDRAID</li> <li>SELINUX</li> <li>STEAM</li> </ul>"},{"location":"tags/#lxc","title":"lxc","text":"<ul> <li>LXC - Image Creation</li> <li>Containers</li> </ul>"},{"location":"tags/#lxd","title":"lxd","text":"<ul> <li>LXC - Image Creation</li> </ul>"},{"location":"tags/#macos","title":"macos","text":"<ul> <li>Containers</li> <li>macOS</li> </ul>"},{"location":"tags/#makemkv","title":"makemkv","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#mariadb","title":"mariadb","text":"<ul> <li>MYSQL</li> </ul>"},{"location":"tags/#markdown","title":"markdown","text":"<ul> <li>Markdown Metadata, its meta-tastic!</li> </ul>"},{"location":"tags/#mdraid","title":"mdraid","text":"<ul> <li>MDRAID</li> </ul>"},{"location":"tags/#metadata","title":"metadata","text":"<ul> <li>Markdown Metadata, its meta-tastic!</li> </ul>"},{"location":"tags/#mtr","title":"mtr","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#mysql","title":"mysql","text":"<ul> <li>MYSQL</li> </ul>"},{"location":"tags/#network","title":"network","text":"<ul> <li>NETWORKING</li> </ul>"},{"location":"tags/#networking","title":"networking","text":"<ul> <li>IPv6</li> <li>MTR</li> <li>NETWORKING</li> </ul>"},{"location":"tags/#openssl","title":"openssl","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#osx","title":"osx","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#podman","title":"podman","text":"<ul> <li>Containers</li> </ul>"},{"location":"tags/#powershell","title":"powershell","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#printers","title":"printers","text":"<ul> <li>CUPS</li> </ul>"},{"location":"tags/#putty","title":"putty","text":"<ul> <li>SSH</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>Python</li> </ul>"},{"location":"tags/#raid","title":"raid","text":"<ul> <li>MDRAID</li> </ul>"},{"location":"tags/#recipes","title":"recipes","text":"<ul> <li>Marry Me Chicken</li> </ul>"},{"location":"tags/#reddit","title":"reddit","text":"<ul> <li>Marry Me Chicken</li> </ul>"},{"location":"tags/#restic","title":"restic","text":"<ul> <li>Restic</li> </ul>"},{"location":"tags/#rhel","title":"rhel","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#rocky","title":"rocky","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#security","title":"security","text":"<ul> <li>SELINUX</li> </ul>"},{"location":"tags/#sed","title":"sed","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#selinux","title":"selinux","text":"<ul> <li>SELINUX</li> </ul>"},{"location":"tags/#shell","title":"shell","text":"<ul> <li>TMUX</li> </ul>"},{"location":"tags/#ssh","title":"ssh","text":"<ul> <li>Ansible</li> <li>SSH</li> <li>TMUX</li> </ul>"},{"location":"tags/#steam","title":"steam","text":"<ul> <li>STEAM</li> </ul>"},{"location":"tags/#storage","title":"storage","text":"<ul> <li>MDRAID</li> </ul>"},{"location":"tags/#sudo","title":"sudo","text":"<ul> <li>macOS</li> </ul>"},{"location":"tags/#svn","title":"svn","text":"<ul> <li>SVN</li> </ul>"},{"location":"tags/#tar","title":"tar","text":"<ul> <li>TAR</li> </ul>"},{"location":"tags/#taskfile","title":"taskfile","text":"<ul> <li>Taskfile.yml</li> </ul>"},{"location":"tags/#tcp","title":"tcp","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#terminal","title":"terminal","text":"<ul> <li>TMUX</li> <li>Taskfile.yml</li> </ul>"},{"location":"tags/#tls","title":"tls","text":"<ul> <li>TLS</li> </ul>"},{"location":"tags/#tmux","title":"tmux","text":"<ul> <li>TMUX</li> </ul>"},{"location":"tags/#traceroute","title":"traceroute","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#ubuntu","title":"ubuntu","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#udp","title":"udp","text":"<ul> <li>MTR</li> </ul>"},{"location":"tags/#vi","title":"vi","text":"<ul> <li>VIM</li> </ul>"},{"location":"tags/#video","title":"video","text":"<ul> <li>FFMPEG</li> </ul>"},{"location":"tags/#vim","title":"vim","text":"<ul> <li>VIM</li> </ul>"},{"location":"tags/#warewulf","title":"warewulf","text":"<ul> <li>HPC</li> <li>Warewulf</li> </ul>"},{"location":"tags/#windows","title":"windows","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#winget","title":"winget","text":"<ul> <li>SSH</li> <li>Windows</li> </ul>"},{"location":"tags/#wip","title":"wip","text":"<ul> <li>K8S</li> <li>SELINUX</li> </ul>"},{"location":"tags/#wtf","title":"wtf","text":"<ul> <li>K8S</li> </ul>"}]}